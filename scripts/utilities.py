import json
import os

import cv2
import numpy as np
import tqdm

from modules.b3d.b3d.external.sort import Sort

DATA_RAW_DIR = '/polyis-data/video-datasets-raw'
DATA_DIR = '/polyis-data/video-datasets-low'
CACHE_DIR = '/polyis-cache'

# Define 10 distinct colors for track visualization (BGR format for OpenCV)
TRACK_COLORS = [
    (255, 0, 0),    # Blue
    (0, 255, 0),    # Green
    (0, 0, 255),    # Red
    (255, 255, 0),  # Cyan
    (255, 0, 255),  # Magenta
    (0, 255, 255),  # Yellow
    (128, 0, 255),  # Purple
    (255, 128, 0),  # Orange
    (0, 128, 255),  # Light Blue
    (255, 0, 128),  # Pink
]


def format_time(**kwargs):
    """
    Format timing information into a list of dictionaries.
    
    Args:
        **kwargs: Keyword arguments where keys are operation names and values are timing values
        
    Returns:
        list: List of dictionaries with 'op' (operation) and 'time' keys for each input argument
        
    Example:
        >>> format_time(read=1.5, detect=2.3)
        [{'op': 'read', 'time': 1.5}, {'op': 'detect', 'time': 2.3}]
    """
    return [{'op': op, 'time': time} for op, time in kwargs.items()]


def load_detection_results(cache_dir: str, dataset: str, video_file: str, tracking: bool = False) -> list[dict]:
    """
    Load detection results from the JSONL file generated by 001_preprocess_groundtruth_detection.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tracking (bool): Whether to load tracking results instead of detection results
        
    Returns:
        list[dict]: list of frame detection results
        
    Raises:
        FileNotFoundError: If no detection results file is found
    """
    file = 'tracking.jsonl' if tracking else 'detections.jsonl'
    detection_path = os.path.join(cache_dir, dataset, video_file, 'groundtruth', file)
    
    if not os.path.exists(detection_path):
        raise FileNotFoundError(f"Detection results not found: {detection_path}")
    
    print(f"Loading detection results from: {detection_path}")
    
    results = []
    with open(detection_path, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    print(f"Loaded {len(results)} frame detections")
    return results


def load_tracking_results(cache_dir: str, dataset: str, video_file: str) -> dict[int, list[list[float]]]:
    """
    Load tracking results from the JSONL file generated by 002_preprocess_groundtruth_tracking.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        
    Returns:
        dict[int, list[list[float]]]: dictionary mapping frame indices to lists of tracks
        
    Raises:
        FileNotFoundError: If no tracking results file is found
    """
    tracking_path = os.path.join(cache_dir, dataset, video_file, 'groundtruth', 'tracking.jsonl')
    
    if not os.path.exists(tracking_path):
        raise FileNotFoundError(f"Tracking results not found: {tracking_path}")
    
    print(f"Loading tracking results from: {tracking_path}")
    
    frame_tracks = {}
    with open(tracking_path, 'r') as f:
        for line in f:
            if line.strip():
                frame_data = json.loads(line)
                frame_idx = frame_data['frame_idx']
                tracks = frame_data['tracks']
                frame_tracks[frame_idx] = tracks
    
    print(f"Loaded tracking results for {len(frame_tracks)} frames")
    return frame_tracks


def interpolate_trajectory(trajectory: list[tuple[int, np.ndarray]], nxt: tuple[int, np.ndarray]) -> list[tuple[int, np.ndarray]]:
    """
    Perform linear interpolation between two trajectory points except the last point (nxt).
    
    Args:
        trajectory (list[tuple[int, np.ndarray]]): list of (frame_idx, detection) tuples
        nxt (tuple[int, np.ndarray]): Next detection point (frame_idx, detection)
        
    Returns:
        list[tuple[int, np.ndarray]]: list of interpolated points
    """
    extend: list[tuple[int, np.ndarray]] = []
    
    if len(trajectory) != 0:
        prv = trajectory[-1]
        assert prv[0] < nxt[0]
        prv_det = prv[1]
        nxt_det = nxt[1]
        dif_det = nxt_det - prv_det
        dif_det = dif_det.reshape(1, -1)

        scale = np.arange(0, nxt[0] - prv[0], dtype=np.float32).reshape(-1, 1) / (nxt[0] - prv[0])
        
        int_dets = (scale @ dif_det) + prv_det.reshape(1, -1)

        for idx, int_det in enumerate(int_dets[:-1]):
            extend.append((prv[0] + idx + 1, int_det))

    return extend


def get_track_color(track_id: int) -> tuple[int, int, int]:
    """
    Get a color for a track ID by cycling through the predefined colors.
    
    Args:
        track_id (int): Track ID
        
    Returns:
        tuple[int, int, int]: BGR color tuple
    """
    color_index = track_id % len(TRACK_COLORS)
    return TRACK_COLORS[color_index]


def overlapi(interval1: tuple[int, int], interval2: tuple[int, int]):
    """
    Check if two 1D intervals overlap.
    
    Args:
        interval1 (tuple[int, int]): First interval as (start, end)
        interval2 (tuple[int, int]): Second interval as (start, end)
        
    Returns:
        bool: True if the intervals overlap, False otherwise
    """
    return (
        (interval1[0] <= interval2[0] <= interval1[1]) or
        (interval1[0] <= interval2[1] <= interval1[1]) or
        (interval2[0] <= interval1[0] <= interval2[1]) or
        (interval2[0] <= interval1[1] <= interval2[1])
    )

def overlap(b1, b2):
    """
    Check if two 2D bounding boxes overlap.
    
    Args:
        b1: First bounding box as (x1, y1, x2, y2) where (x1, y1) is top-left and (x2, y2) is bottom-right
        b2: Second bounding box as (x1, y1, x2, y2) where (x1, y1) is top-left and (x2, y2) is bottom-right
        
    Returns:
        bool: True if the bounding boxes overlap in both x and y dimensions, False otherwise
    """
    return overlapi((b1[0], b1[2]), (b2[0], b2[2])) and overlapi((b1[1], b1[3]), (b2[1], b2[3]))


def load_classification_results(cache_dir: str, dataset: str, video_file: str, tile_size: int, classifier: str, groundtruth: bool = False) -> list:
    """
    Load classification results from the JSONL file generated by 020_exec_classify.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tile_size (int): Tile size used for classification
        classifier (str): Classifier name to use
        groundtruth (bool): Whether to use groundtruth scores (score_correct.jsonl) instead of model scores (score.jsonl)
        
    Returns:
        list: List of frame classification results, each containing frame data and classifications
        
    Raises:
        FileNotFoundError: If no classification results file is found
    """
    # Look for the classification results file
    score_dir = os.path.join(cache_dir, dataset, video_file, 'relevancy', f'{classifier}_{tile_size}', 'score')
    
    # Determine which filename to look for based on groundtruth flag
    if groundtruth:
        expected_filename = 'score_correct.jsonl'
    else:
        expected_filename = 'score.jsonl'
    
    # Look for the specific results file
    results_file = os.path.join(score_dir, expected_filename)
    
    if not os.path.exists(results_file):
        raise FileNotFoundError(f"Classification results file not found: {results_file}")
    
    print(f"Loading classification results from: {results_file}")
    
    results = []
    with open(results_file, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    print(f"Loaded {len(results)} frame classifications")
    return results


def create_tracker(tracker_name: str, max_age: int = 1, min_hits: int = 3, iou_threshold: float = 0.3):
    """
    Create a tracker instance based on the specified algorithm.
    
    Args:
        tracker_name (str): Name of the tracking algorithm
        max_age (int): Maximum age for SORT tracker
        min_hits (int): Minimum hits for SORT tracker
        iou_threshold (float): IOU threshold for SORT tracker
        
    Returns:
        Tracker instance
        
    Raises:
        ValueError: If the tracker name is not supported
    """
    if tracker_name == 'sort':
        print(f"Creating SORT tracker with max_age={max_age}, min_hits={min_hits}, iou_threshold={iou_threshold}")
        return Sort(max_age=max_age, min_hits=min_hits, iou_threshold=iou_threshold)
    else:
        raise ValueError(f"Unknown tracker: {tracker_name}")


def create_visualization_frame(frame: np.ndarray, tracks: list[list[float]], 
                             frame_idx: int, trajectory_history: dict[int, list[tuple[int, int, int]]], 
                             speed_up: int) -> np.ndarray | None:
    """
    Create a visualization frame by drawing bounding boxes and trajectories for all tracks.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        tracks (list[list[float]]): list of tracks for this frame
        frame_idx (int): Frame index for logging
        trajectory_history (dict[int, list[tuple[int, int, int]]]): History of track centers with frame timestamps
        speed_up (int): Speed up factor (process every Nth frame)
        
    Returns:
        np.ndarray | None: Frame with bounding boxes and trajectories drawn, or None if frame should be skipped
    """
    # First loop: Update trajectory history for all tracks
    for track in tracks:
        if len(track) >= 5:  # Ensure we have track_id, x1, y1, x2, y2
            track_id, x1, y1, x2, y2 = track[:5]
            track_id = int(track_id)
            
            # Calculate center of bounding box
            center_x = int((x1 + x2) // 2)
            center_y = int((y1 + y2) // 2)
            
            # Update trajectory history with frame timestamp
            if track_id not in trajectory_history:
                trajectory_history[track_id] = []
            trajectory_history[track_id].append((center_x, center_y, frame_idx))

    if frame_idx % speed_up != 0:
        return None
    
    # Create a copy of the frame for visualization
    vis_frame = frame.copy()
    
    # Second loop: Draw bounding boxes and labels for current tracks
    for track in tracks:
        if len(track) >= 5:  # Ensure we have track_id, x1, y1, x2, y2
            track_id, x1, y1, x2, y2 = track[:5]
            
            # Convert to integers for drawing
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            track_id = int(track_id)
            
            # Get color for this track
            color = get_track_color(track_id)
            
            # Draw bounding box
            cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, 2)
            
            # Draw track ID label
            label = f"ID: {track_id}"
            font_scale = 0.6
            font_thickness = 2
            
            # Calculate text size and position
            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 
                                                                 font_scale, font_thickness)
            
            # Position text above the bounding box
            text_x = x1
            text_y = max(y1 - 10, text_height + 5)
            
            # Draw text background for better visibility
            cv2.rectangle(vis_frame, (text_x - 2, text_y - text_height - 2), 
                         (text_x + text_width + 2, text_y + baseline + 2), 
                         color, -1)
            
            # Draw text
            cv2.putText(vis_frame, label, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)
    
    # Draw all trajectories with gradual fading
    for track_id, trajectory in trajectory_history.items():
        if len(trajectory) > 1:
            color = get_track_color(track_id)
            
            # Calculate fade parameters
            max_fade_frames = 30  # Number of frames for complete fade after track ends
            current_time = frame_idx
            
            # Check if track is still active (within last 5 frames)
            track_is_active = trajectory and current_time - trajectory[-1][2] <= 5
            
            # Calculate fade alpha for the entire trajectory
            if track_is_active:
                # Track is active - full opacity
                alpha = 1.0
            else:
                # Track has ended - calculate fade based on time since last detection
                time_since_end = current_time - trajectory[-1][2]
                if time_since_end >= max_fade_frames:
                    alpha = 0.0  # Completely faded
                else:
                    alpha = 1.0 - (time_since_end / max_fade_frames)
            
            # Only draw if trajectory is still visible
            if alpha > 0.01:
                # Apply alpha to color for the entire trajectory
                line_color = tuple(int(c * alpha) for c in color)
                point_color = tuple(int(c * alpha) for c in color)
                
                # Draw trajectory lines
                for i in range(1, len(trajectory)):
                    prev_center = trajectory[i-1]
                    curr_center = trajectory[i]
                    
                    # Draw line
                    cv2.line(vis_frame, (prev_center[0], prev_center[1]), 
                             (curr_center[0], curr_center[1]), line_color, 2)
                    
                    # Draw trajectory points
                    point_radius = max(1, int(3 * alpha))
                    cv2.circle(vis_frame, (prev_center[0], prev_center[1]), point_radius, point_color, -1)
                
                # Draw final point
                final_center = trajectory[-1]
                cv2.circle(vis_frame, (final_center[0], final_center[1]), 3, point_color, -1)
    
    return vis_frame


def create_tracking_visualization(video_path: str, tracking_results: dict[int, list[list[float]]], 
                                 output_path: str, speed_up: int, process_id: int):
    """
    Create a visualization video showing tracking results overlaid on the original video.
    
    Args:
        video_path (str): Path to the input video file
        tracking_results (dict[int, list[list[float]]]): Tracking results from load_tracking_results
        output_path (str): Path where the output visualization video will be saved
    """
    print(f"Creating tracking visualization for video: {video_path}")
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return
    
    # Get video properties
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"Video info: {width}x{height}, {fps} FPS, {frame_count} frames")
    
    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_path)
    os.makedirs(output_dir, exist_ok=True)
    
    # Create video writer
    fourcc = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')
    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    if not writer.isOpened():
        print(f"Error: Could not create video writer for {output_path}")
        cap.release()
        return
    
    print(f"Creating visualization video with {frame_count} frames at {fps} FPS")
    
    # Initialize trajectory history for all tracks with frame timestamps
    trajectory_history: dict[int, list[tuple[int, int, int]]] = {}  # track_id -> [(x, y, frame_idx), ...]
    
    # Initialize frame_idx for exception handling
    frame_idx = 0
    
    # Process each frame
    try:
        for frame_idx in tqdm.tqdm(range(frame_count), desc=f"Process {process_id} - Creating visualization", position=process_id):
            # Read frame
            ret, frame = cap.read()
            if not ret:
                break
            
            # Get tracking results for this frame
            tracks = tracking_results.get(frame_idx, [])
            
            # Create visualization frame with trajectory history
            vis_frame = create_visualization_frame(frame, tracks, frame_idx, trajectory_history, speed_up)
            
            # Write frame to video
            if vis_frame is not None:
                writer.write(vis_frame)
    
    except KeyboardInterrupt:
        print(f"\nProcess {process_id}: KeyboardInterrupt detected. Stopping video writing...")
        print(f"Process {process_id}: Video writing stopped at frame {frame_idx}")
    except Exception as e:
        print(f"\nProcess {process_id}: Error during video processing: {e}")
    finally:
        # Release resources
        cap.release()
        writer.release()
        print(f"Process {process_id}: Resources released")
    
    print(f"Process {process_id}: Tracking visualization completed")