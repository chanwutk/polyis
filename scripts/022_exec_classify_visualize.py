#!/usr/local/bin/python

import argparse
import json
import os
import cv2
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from typing import Any
import multiprocessing as mp
from functools import partial


DATA_DIR = '/polyis-data/video-datasets-low'
CACHE_DIR = '/polyis-cache'
TILE_SIZES = [32, 64, 128]


def parse_args():
    """
    Parse command line arguments for the script.
    
    Returns:
        argparse.Namespace: Parsed command line arguments containing:
            - dataset (str): Dataset name to process (default: 'b3d')
            - tile_size (int | str): Tile size to use for classification (choices: 32, 64, 128, 'all')
            - threshold (float): Threshold for classification visualization (default: 0.5)
            - groundtruth (bool): Whether to use groundtruth scores (score_correct.jsonl) instead of model scores (score.jsonl)
            - statistics (bool): Whether to compare classification results with groundtruth and generate statistics
    """
    parser = argparse.ArgumentParser(description='Visualize video tile classification results')
    parser.add_argument('--dataset', required=False,
                        default='b3d',
                        help='Dataset name')
    parser.add_argument('--tile_size', type=str, choices=['32', '64', '128', 'all'], default='all',
                        help='Tile size to use for classification (or "all" for all tile sizes)')
    parser.add_argument('--threshold', type=float, default=0.5,
                        help='Threshold for classification visualization (0.0 to 1.0)')
    parser.add_argument('--groundtruth', action='store_true',
                        help='Use groundtruth scores (score_correct.jsonl) instead of model scores (score.jsonl)')
    parser.add_argument('--statistics', action='store_true',
                        help='Compare classification results with groundtruth and generate statistics (no video output, ignores --groundtruth flag)')
    return parser.parse_args()


def load_classification_results(cache_dir: str, dataset: str, video_file: str, tile_size: int, groundtruth: bool = False) -> list:
    """
    Load classification results from the JSONL file generated by 020_exec_classify.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tile_size (int): Tile size used for classification
        groundtruth (bool): Whether to use groundtruth scores (score_correct.jsonl) instead of model scores (score.jsonl)
        
    Returns:
        list: list of frame classification results, each containing frame data and classifications
        
    Raises:
        FileNotFoundError: If no classification results file is found
    """
    # Look for the classification results file
    score_dir = os.path.join(cache_dir, dataset, video_file, 'relevancy', 'score', f'proxy_{tile_size}')
    
    # Determine which filename to look for based on groundtruth flag
    if groundtruth:
        expected_filename = 'score_correct.jsonl'
    else:
        # Find any file that starts with "score", ends with ".jsonl", but is not "score_correct.jsonl"
        possible_files = [f for f in os.listdir(score_dir)
                          if f.startswith('score') and f.endswith('.jsonl') and f != 'score_correct.jsonl']
        if not possible_files:
            raise FileNotFoundError(f"No classification results file found in {score_dir} (excluding score_correct.jsonl)")
        expected_filename = possible_files[0]
    
    # Look for the specific results file
    results_file = os.path.join(score_dir, expected_filename)
    
    if not os.path.exists(results_file):
        raise FileNotFoundError(f"Classification results file not found: {results_file}")
    
    print(f"Loading classification results from: {results_file}")
    
    results = []
    with open(results_file, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    print(f"Loaded {len(results)} frame classifications")
    return results


def load_groundtruth_detections(cache_dir: str, dataset: str, video_file: str) -> list[dict]:
    """
    Load groundtruth detection annotations for a video.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        
    Returns:
        list[list[dict]]: list of frame detections, each frame containing a list of detection dictionaries
                          with 'bbox' (x1, y1, x2, y2) and other annotation data
        
    Raises:
        FileNotFoundError: If no groundtruth file is found
    """
    # Look for groundtruth annotations file
    gt_dir = os.path.join(cache_dir, dataset, video_file, 'groundtruth')
    gt_file = os.path.join(gt_dir, 'tracking.jsonl')
    
    if not os.path.exists(gt_file):
        raise FileNotFoundError(f"Groundtruth file not found: {gt_file}")
    
    print(f"Loading groundtruth detections from: {gt_file}")
    
    detections = []
    with open(gt_file, 'r') as f:
        for line in f:
            if line.strip():
                detections.append(json.loads(line))
    
    print(f"Loaded {len(detections)} frame detections")
    return detections





def evaluate_classification_accuracy(classifications: list[list[float]], 
                                   detections: list[list[float]], 
                                   tile_size: int, threshold: float) -> dict[str, Any]:
    """
    Evaluate classification accuracy by comparing predictions with groundtruth detections.
    
    Args:
        classifications (list[list[float]]): 2D grid of classification scores
        detections (list[dict]): list of detection dictionaries
        tile_size (int): Size of each tile
        threshold (float): Classification threshold
        
    Returns:
        dict: dictionary containing evaluation metrics and error details
    """
    grid_height = len(classifications)
    grid_width = len(classifications[0]) if grid_height > 0 else 0
    
    # Initialize counters
    tp = 0  # True Positive: predicted above threshold, has detection overlap
    tn = 0  # True Negative: predicted below threshold, no detection overlap
    fp = 0  # False Positive: predicted above threshold, no detection overlap
    fn = 0  # False Negative: predicted below threshold, has detection overlap

    # Create a bitmap for all detections first
    # Calculate the total image dimensions based on grid and tile size
    total_height = grid_height * tile_size
    total_width = grid_width * tile_size
    detection_bitmap = np.zeros((total_height, total_width), dtype=np.uint32)
    
    # Mark all detections on the bitmap
    for detection in detections:
        if len(detection) != 5:
            continue
            
        # bbox format: [track_id, x1, y1, x2, y2]
        _track_id, det_x1, det_y1, det_x2, det_y2 = detection
        
        # Convert to integer coordinates and ensure they're within bitmap bounds
        det_x1 = max(0, int(det_x1))
        det_y1 = max(0, int(det_y1))
        det_x2 = min(total_width, int(det_x2))
        det_y2 = min(total_height, int(det_y2))
        
        assert det_x2 > det_x1 and det_y2 > det_y1, f"Invalid detection: {detection}"
        detection_bitmap[det_y1:det_y2, det_x1:det_x2] = 1
    
    error_map = np.zeros((grid_height, grid_width), dtype=int)
    overlap_ratios = []
    classification_scores = []
    
    # Extract tile regions from the detection bitmap and calculate overlap ratios
    for i in range(grid_height):
        for j in range(grid_width):
            score = classifications[i][j]
            
            # Calculate tile boundaries in the bitmap
            tile_start_y = i * tile_size
            tile_end_y = tile_start_y + tile_size
            tile_start_x = j * tile_size
            tile_end_x = tile_start_x + tile_size
            
            # Extract tile region from detection bitmap
            tile_region = detection_bitmap[tile_start_y:tile_end_y, tile_start_x:tile_end_x]
            
            # Calculate overlap ratio: count of 1s divided by total pixels
            overlap = float(np.sum(tile_region)) / (tile_size * tile_size)
            
            # Store data for scatter plot
            overlap_ratios.append(overlap)
            classification_scores.append(score)
            
            # Determine prediction
            predicted_positive = score >= threshold
            actual_positive = overlap > 0.0
            
            # Count metrics
            if predicted_positive and actual_positive:
                tp += 1
            elif predicted_positive and not actual_positive:
                fp += 1
                error_map[i, j] = 1  # False positive error
            elif not predicted_positive and actual_positive:
                fn += 1
                error_map[i, j] = 2  # False negative error
            else:
                tn += 1
    
    # Calculate precision, recall, and accuracy
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,
        'precision': precision, 'recall': recall, 'accuracy': accuracy, 'f1_score': f1_score,
        'error_map': error_map,
        'overlap_ratios': overlap_ratios,
        'classification_scores': classification_scores,
        'total_tiles': grid_height * grid_width
    }


def _evaluate_frame_worker(args):
    """
    Worker function to evaluate a single frame for multiprocessing.
    
    Args:
        args: Tuple containing (frame_result, frame_detections, tile_size, threshold)
        
    Returns:
        dict: Frame evaluation results
    """
    frame_result, frame_detections, tile_size, threshold = args
    
    # Validate frame data
    assert 'tracks' in frame_detections, f"tracks not in frame_detections: {frame_detections}"
    
    classifications = frame_result['tile_classifications']
    
    # Evaluate this frame
    frame_eval = evaluate_classification_accuracy(
        classifications, frame_detections['tracks'], tile_size, threshold
    )
    
    return frame_eval


def create_statistics_visualizations(video_file: str, results: list[dict], 
                                   groundtruth_detections: list[dict], 
                                   tile_size: int, threshold: float, 
                                   output_dir: str):
    """
    Create comprehensive statistics visualizations comparing classification results with groundtruth.
    
    Args:
        video_file (str): Name of the video file
        results (list[dict]): Classification results
        groundtruth_detections (list[list[dict]]): Groundtruth detections per frame
        tile_size (int): Tile size used for classification
        threshold (float): Classification threshold
        output_dir (str): Directory to save visualizations
    """
    print(f"Creating statistics visualizations for {video_file}")
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Collect frame-by-frame metrics
    frame_metrics = []
    all_overlap_ratios = []
    all_classification_scores = []
    all_error_counts = []
    
    print(f"Evaluating {len(results)} frames using multiprocessing")
    
    # Prepare arguments for parallel processing
    # Validate frame indices first
    for frame_idx, (frame_result, frame_detections) in enumerate(zip(results, groundtruth_detections)):
        assert frame_idx == frame_result['frame_idx'], f"frame_idx mismatch: {frame_idx} != {frame_result['frame_idx']}"
    
    # Create arguments for worker function
    worker_args = [(frame_result, frame_detections, tile_size, threshold) 
                   for frame_result, frame_detections in zip(results, groundtruth_detections)]
    
    # Use multiprocessing to evaluate frames in parallel
    num_processes = min(mp.cpu_count(), len(results))
    with mp.Pool(processes=num_processes) as pool:
        frame_evals = list(tqdm(
            pool.imap(_evaluate_frame_worker, worker_args),
            total=len(results),
            desc="Evaluating frames"
        ))
    
    # Collect results from parallel processing
    for frame_eval in frame_evals:
        frame_metrics.append(frame_eval)
        all_overlap_ratios.extend(frame_eval['overlap_ratios'])
        all_classification_scores.extend(frame_eval['classification_scores'])
        all_error_counts.append(frame_eval['error_map'])
    
    # Aggregate overall metrics
    total_tp = sum(m['tp'] for m in frame_metrics)
    total_tn = sum(m['tn'] for m in frame_metrics)
    total_fp = sum(m['fp'] for m in frame_metrics)
    total_fn = sum(m['fn'] for m in frame_metrics)
    
    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
    overall_accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn) if (total_tp + total_tn + total_fp + total_fn) > 0 else 0.0
    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0
    
    # 1. Overall classification error summary
    fig, ((ax1, ax3), (ax2, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # First stacked bar chart: x-axis is Predicted, color is Actual
    x_labels = ['Predicted Negative', 'Predicted Positive']
    actual_negative_values = [total_tn, total_fp]  # TN, FP
    actual_positive_values = [total_fn, total_tp]  # FN, TP
    
    bars1 = ax1.bar(x_labels, actual_negative_values, label='Actual Negative', color='lightcoral', alpha=0.8)
    bars2 = ax1.bar(x_labels, actual_positive_values, bottom=actual_negative_values, label='Actual Positive', color='lightgreen', alpha=0.8)
    
    # Add value labels on bars
    for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):
        # Label for Actual Negative (bottom)
        if actual_negative_values[i] > 0:
            ax1.text(bar1.get_x() + bar1.get_width()/2, bar1.get_height()/2, 
                    str(actual_negative_values[i]), ha='center', va='center', fontweight='bold')
        
        # Label for Actual Positive (top)
        if actual_positive_values[i] > 0:
            ax1.text(bar2.get_x() + bar2.get_width()/2, bar2.get_y() + bar2.get_height()/2, 
                    str(actual_positive_values[i]), ha='center', va='center', fontweight='bold')
    
    ax1.set_ylabel('Count')
    ax1.set_title(f'Classification Results by Prediction (Tile Size: {tile_size})')
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')
    
    # Metrics bar chart
    metrics = ['Precision', 'Recall', 'Accuracy', 'F1-Score']
    values = [overall_precision, overall_recall, overall_accuracy, overall_f1]
    colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']
    bars = ax2.bar(metrics, values, color=colors, alpha=0.7)
    ax2.set_ylabel('Score')
    ax2.set_title(f'Overall Classification Metrics (Tile Size: {tile_size})')
    ax2.set_ylim(0, 1)
    for bar, value in zip(bars, values):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{value:.3f}', ha='center', va='bottom')
    
    # Second stacked bar chart: x-axis is Actual, color is Predicted
    x_labels2 = ['Actual Negative', 'Actual Positive']
    predicted_negative_values = [total_tn, total_fn]  # TN, FN
    predicted_positive_values = [total_fp, total_tp]  # FP, TP
    
    bars3 = ax3.bar(x_labels2, predicted_negative_values, label='Predicted Negative', color='lightcoral', alpha=0.8)
    bars4 = ax3.bar(x_labels2, predicted_positive_values, bottom=predicted_negative_values, label='Predicted Positive', color='lightgreen', alpha=0.8)
    
    # Add value labels on bars
    for i, (bar3, bar4) in enumerate(zip(bars3, bars4)):
        # Label for Predicted Negative (bottom)
        if predicted_negative_values[i] > 0:
            ax3.text(bar3.get_x() + bar3.get_width()/2, bar3.get_height()/2, 
                    str(predicted_negative_values[i]), ha='center', va='center', fontweight='bold')
        
        # Label for Predicted Positive (top)
        if predicted_positive_values[i] > 0:
            ax3.text(bar4.get_x() + bar4.get_width()/2, bar4.get_y() + bar4.get_height()/2, 
                    str(predicted_positive_values[i]), ha='center', va='center', fontweight='bold')
    
    ax3.set_ylabel('Count')
    ax3.set_title(f'Classification Results by Actual (Tile Size: {tile_size})')
    ax3.legend()
    ax3.grid(True, alpha=0.3, axis='y')
    
    # Statistics table
    stats_text = f"""
    Total Tiles: {sum(m['total_tiles'] for m in frame_metrics):,}
    
    True Positives: {total_tp:,}
    True Negatives: {total_tn:,}
    False Positives: {total_fp:,}
    False Negatives: {total_fn:,}
    
    Precision: {overall_precision:.4f}
    Recall: {overall_recall:.4f}
    Accuracy: {overall_accuracy:.4f}
    F1-Score: {overall_f1:.4f}
    
    Threshold: {threshold}
    """
    
    ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.set_title(f'Overall Statistics Summary (Tile Size: {tile_size})')
    ax4.axis('off')
    
    plt.tight_layout()
    overall_summary_path = os.path.join(output_dir, f'010_overall_summary_tile{tile_size}.png')
    plt.savefig(overall_summary_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. Classification error over time
    plt.figure(figsize=(15, 8))
    
    frame_indices = list(range(len(frame_metrics)))
    error_rates = [(m['fp'] + m['fn']) / m['total_tiles'] for m in frame_metrics]
    precision_rates = [m['precision'] for m in frame_metrics]
    recall_rates = [m['recall'] for m in frame_metrics]
    
    # Calculate number of objects per frame (detections with overlap > 0)
    objects_per_frame = []
    for frame_detection in groundtruth_detections:
        assert 'tracks' in frame_detection, f"Frame detection does not contain tracks: {frame_detection}"
        # Count detections that have any overlap with tiles
        object_count = len([det for det in frame_detection['tracks'] if len(det) == 5])
        objects_per_frame.append(object_count)
    
    plt.subplot(2, 1, 1)
    ax1 = plt.gca()
    ax2 = ax1.twinx()
    
    # Plot error rate on primary y-axis
    line1 = ax1.plot(frame_indices, error_rates, 'r-', linewidth=2, label='Error Rate')
    mean_error = float(np.mean(error_rates))
    ax1.axhline(y=mean_error, color='red', linestyle='--', alpha=0.7, label=f'Mean Error: {mean_error:.3f}')
    ax1.set_xlabel('Frame Index')
    ax1.set_ylabel('Error Rate', color='red')
    ax1.tick_params(axis='y', labelcolor='red')
    ax1.set_title(f'Classification Error Rate Over Time (Tile Size: {tile_size})')
    ax1.grid(True, alpha=0.3)
    
    # Plot object count on secondary y-axis
    line2 = ax2.plot(frame_indices, objects_per_frame, 'purple', linewidth=2, label='Object Count', alpha=0.7)
    ax2.set_ylabel('Object Count', color='purple')
    ax2.tick_params(axis='y', labelcolor='purple')
    
    # Combine legends
    lines = line1 + line2
    labels = [str(l.get_label()) for l in lines]
    ax1.legend(lines, labels, loc='upper right')
    
    plt.subplot(2, 1, 2)
    ax3 = plt.gca()
    ax4 = ax3.twinx()
    
    # Plot precision and recall on primary y-axis
    line3 = ax3.plot(frame_indices, precision_rates, 'g-', linewidth=2, label='Precision')
    line4 = ax3.plot(frame_indices, recall_rates, 'b-', linewidth=2, label='Recall')
    ax3.axhline(y=float(overall_precision), color='green', linestyle='--', alpha=0.7, label=f'Overall Precision: {overall_precision:.3f}')
    ax3.axhline(y=float(overall_recall), color='blue', linestyle='--', alpha=0.7, label=f'Overall Recall: {overall_recall:.3f}')
    ax3.set_xlabel('Frame Index')
    ax3.set_ylabel('Score')
    ax3.set_title(f'Precision and Recall Over Time (Tile Size: {tile_size})')
    ax3.grid(True, alpha=0.3)
    
    # Plot object count on secondary y-axis
    line5 = ax4.plot(frame_indices, objects_per_frame, 'purple', linewidth=2, label='Object Count', alpha=0.7)
    ax4.set_ylabel('Object Count', color='purple')
    ax4.tick_params(axis='y', labelcolor='purple')
    
    # Combine legends
    lines = line3 + line4 + line5
    labels = [str(l.get_label()) for l in lines]
    ax3.legend(lines, labels, loc='upper right')
    
    plt.tight_layout()
    time_series_path = os.path.join(output_dir, f'020_time_series_tile{tile_size}.png')
    plt.savefig(time_series_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. Heatmap of error count for each tile
    # Aggregate error maps across all frames
    grid_height = len(frame_metrics[0]['error_map'])
    grid_width = len(frame_metrics[0]['error_map'][0])
    aggregated_error_map = np.zeros((grid_height, grid_width), dtype=int)
    
    for error_map in all_error_counts:
        aggregated_error_map += error_map
    
    plt.figure(figsize=(12, 8))
    # Use matplotlib heatmap
    plt.imshow(aggregated_error_map, cmap='Reds', interpolation='nearest')
    for i in range(aggregated_error_map.shape[0]):
        for j in range(aggregated_error_map.shape[1]):
            if aggregated_error_map[i, j] == 0:
                plt.text(j, i, str(aggregated_error_map[i, j]), 
                         ha='center', va='center', color='black', fontweight='bold')
    plt.colorbar(label='Error Count')
    
    plt.title(f'Cumulative Error Count per Tile (Tile Size: {tile_size})\nRed: False Positive, Orange: False Negative')
    plt.xlabel('Tile X Position')
    plt.ylabel('Tile Y Position')
    
    error_heatmap_path = os.path.join(output_dir, f'030_error_heatmap_tile{tile_size}.png')
    plt.savefig(error_heatmap_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. Heatmaps: Classification Score vs Detection Overlap (split by correctness)
    
    # Separate data for correct and incorrect predictions
    correct_scores = []
    correct_overlaps = []
    incorrect_scores = []
    incorrect_overlaps = []
    
    for score, overlap in zip(all_classification_scores, all_overlap_ratios):
        predicted_positive = score >= threshold
        actual_positive = overlap > 0.0
        if predicted_positive == actual_positive:
            correct_scores.append(score)
            correct_overlaps.append(overlap)
        else:
            incorrect_scores.append(score)
            incorrect_overlaps.append(overlap)
    
    # Create heatmap for correct predictions
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    if correct_scores and correct_overlaps:
        # Create 2D histogram for correct predictions
        H_correct, xedges_correct, yedges_correct = np.histogram2d(
            correct_scores, correct_overlaps, bins=50, 
            range=[[0, 1], [0, 1]]
        )
        
        # Plot heatmap with log scale
        # Add small value to avoid log(0)
        H_correct_log = np.log10(H_correct + 1)
        plt.imshow(H_correct_log.T, origin='lower', extent=(0, 1, 0, 1), 
                   cmap='Greens', aspect='auto', interpolation='nearest')
        plt.colorbar(label='Log10(Point Count + 1)')
        plt.axvline(x=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
        plt.axhline(y=0.0, color='red', linestyle='--', linewidth=1, alpha=0.7)
        plt.xlabel('Classification Score')
        plt.ylabel('Detection Overlap Ratio')
        plt.title(f'Correct Predictions (Tile Size: {tile_size})\nTotal: {len(correct_scores):,}')
        plt.legend()
    else:
        plt.text(0.5, 0.5, 'No correct predictions', ha='center', va='center', transform=plt.gca().transAxes)
        plt.title(f'Correct Predictions (Tile Size: {tile_size})')
    
    # Create heatmap for incorrect predictions
    plt.subplot(1, 2, 2)
    if incorrect_scores and incorrect_overlaps:
        # Create 2D histogram for incorrect predictions
        H_incorrect, xedges_incorrect, yedges_incorrect = np.histogram2d(
            incorrect_scores, incorrect_overlaps, bins=50, 
            range=[[0, 1], [0, 1]]
        )
        
        # Plot heatmap with log scale
        # Add small value to avoid log(0)
        H_incorrect_log = np.log10(H_incorrect + 1)
        plt.imshow(H_incorrect_log.T, origin='lower', extent=(0, 1, 0, 1), 
                   cmap='Reds', aspect='auto', interpolation='nearest')
        plt.colorbar(label='Log10(Point Count + 1)')
        plt.axvline(x=threshold, color='blue', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
        plt.axhline(y=0.0, color='blue', linestyle='--', linewidth=1, alpha=0.7)
        plt.xlabel('Classification Score')
        plt.ylabel('Detection Overlap Ratio')
        plt.title(f'Incorrect Predictions (Tile Size: {tile_size})\nTotal: {len(incorrect_scores):,}')
        plt.legend()
    else:
        plt.text(0.5, 0.5, 'No incorrect predictions', ha='center', va='center', transform=plt.gca().transAxes)
        plt.title(f'Incorrect Predictions (Tile Size: {tile_size})')
    
    plt.tight_layout()
    scatter_path = os.path.join(output_dir, f'040_heatmap_overlap_tile{tile_size}.png')
    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # 5. Detailed metrics plot
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # Precision, Recall, F1 over time
    ax1_twin = ax1.twinx()
    line1 = ax1.plot(frame_indices, precision_rates, 'g-', linewidth=2, label='Precision')
    line2 = ax1.plot(frame_indices, recall_rates, 'b-', linewidth=2, label='Recall')
    line3 = ax1.plot(frame_indices, [m['f1_score'] for m in frame_metrics], 'orange', linewidth=2, label='F1-Score')
    ax1.axhline(y=float(overall_precision), color='green', linestyle='--', alpha=0.7)
    ax1.axhline(y=float(overall_recall), color='blue', linestyle='--', alpha=0.7)
    ax1.axhline(y=float(overall_f1), color='orange', linestyle='--', alpha=0.7)
    ax1.set_xlabel('Frame Index')
    ax1.set_ylabel('Score')
    ax1.set_title(f'Precision, Recall, F1 Over Time (Tile Size: {tile_size})')
    ax1.grid(True, alpha=0.3)
    
    # Object count on secondary y-axis
    line4 = ax1_twin.plot(frame_indices, objects_per_frame, 'purple', linewidth=2, label='Object Count', alpha=0.7)
    ax1_twin.set_ylabel('Object Count', color='purple')
    ax1_twin.tick_params(axis='y', labelcolor='purple')
    
    # Combine legends
    lines = line1 + line2 + line3 + line4
    labels = [str(l.get_label()) for l in lines]
    ax1.legend(lines, labels, loc='upper right')
    
    # True Positive, True Negative over time
    tp_counts = [m['tp'] for m in frame_metrics]
    tn_counts = [m['tn'] for m in frame_metrics]
    ax2_twin = ax2.twinx()
    line5 = ax2.plot(frame_indices, tp_counts, 'g-', linewidth=2, label='True Positives')
    line6 = ax2.plot(frame_indices, tn_counts, 'b-', linewidth=2, label='True Negatives')
    ax2.set_xlabel('Frame Index')
    ax2.set_ylabel('Count')
    ax2.set_title(f'True Positives and Negatives Over Time (Tile Size: {tile_size})')
    ax2.grid(True, alpha=0.3)
    
    # Object count on secondary y-axis
    line7 = ax2_twin.plot(frame_indices, objects_per_frame, 'purple', linewidth=2, label='Object Count', alpha=0.7)
    ax2_twin.set_ylabel('Object Count', color='purple')
    ax2_twin.tick_params(axis='y', labelcolor='purple')
    
    # Combine legends
    lines = line5 + line6 + line7
    labels = [str(l.get_label()) for l in lines]
    ax2.legend(lines, labels, loc='upper right')
    
    # False Positive, False Negative over time
    fp_counts = [m['fp'] for m in frame_metrics]
    fn_counts = [m['fn'] for m in frame_metrics]
    ax3_twin = ax3.twinx()
    line8 = ax3.plot(frame_indices, fp_counts, 'r-', linewidth=2, label='False Positives')
    line9 = ax3.plot(frame_indices, fn_counts, 'orange', linewidth=2, label='False Negatives')
    ax3.set_xlabel('Frame Index')
    ax3.set_ylabel('Count')
    ax3.set_title(f'False Positives and Negatives Over Time (Tile Size: {tile_size})')
    ax3.grid(True, alpha=0.3)
    
    # Object count on secondary y-axis
    line10 = ax3_twin.plot(frame_indices, objects_per_frame, 'purple', linewidth=2, label='Object Count', alpha=0.7)
    ax3_twin.set_ylabel('Object Count', color='purple')
    ax3_twin.tick_params(axis='y', labelcolor='purple')
    
    # Combine legends
    lines = line8 + line9 + line10
    labels = [str(l.get_label()) for l in lines]
    ax3.legend(lines, labels, loc='upper right')
    
    # Error rate breakdown
    fp_rates = [m['fp'] / m['total_tiles'] for m in frame_metrics]
    fn_rates = [m['fn'] / m['total_tiles'] for m in frame_metrics]
    ax4_twin = ax4.twinx()
    line11 = ax4.plot(frame_indices, fp_rates, 'r-', linewidth=2, label='False Positive Rate')
    line12 = ax4.plot(frame_indices, fn_rates, 'orange', linewidth=2, label='False Negative Rate')
    ax4.set_xlabel('Frame Index')
    ax4.set_ylabel('Rate')
    ax4.set_title(f'False Positive and Negative Rates Over Time (Tile Size: {tile_size})')
    ax4.grid(True, alpha=0.3)
    
    # Object count on secondary y-axis
    line13 = ax4_twin.plot(frame_indices, objects_per_frame, 'purple', linewidth=2, label='Object Count', alpha=0.7)
    ax4_twin.set_ylabel('Object Count', color='purple')
    ax4_twin.tick_params(axis='y', labelcolor='purple')
    
    # Combine legends
    lines = line11 + line12 + line13
    labels = [str(l.get_label()) for l in lines]
    ax4.legend(lines, labels, loc='upper right')
    
    plt.tight_layout()
    detailed_metrics_path = os.path.join(output_dir, f'050_detailed_metrics_tile{tile_size}.png')
    plt.savefig(detailed_metrics_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved statistics visualizations to: {output_dir}")
    print(f"Overall Metrics - Precision: {overall_precision:.3f}, Recall: {overall_recall:.3f}, F1: {overall_f1:.3f}")


def create_visualization_frame(frame: np.ndarray, classifications: list[list[float]], 
                              tile_size: int, threshold: float) -> np.ndarray:
    """
    Create a visualization frame by adjusting tile brightness based on classification scores.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        classifications (list[list[float]]): 2D grid of classification scores
        tile_size (int): Size of tiles used for classification
        threshold (float): Threshold value for visualization
        
    Returns:
        np.ndarray: Visualization frame with adjusted tile brightness
    """
    # Create a copy of the frame for visualization
    vis_frame = frame.copy().astype(np.float32)
    
    # Get grid dimensions
    grid_height = len(classifications)
    grid_width = len(classifications[0]) if grid_height > 0 else 0
    
    # Calculate frame dimensions after padding
    vis_height = grid_height * tile_size
    vis_width = grid_width * tile_size
    
    # Ensure frame is large enough (handle padding)
    if frame.shape[0] < vis_height or frame.shape[1] < vis_width:
        # Pad frame if necessary
        pad_height = max(0, vis_height - frame.shape[0])
        pad_width = max(0, vis_width - frame.shape[1])
        vis_frame = np.pad(vis_frame, ((0, pad_height), (0, pad_width), (0, 0)), mode='constant')
    
    # Apply brightness adjustments to each tile
    for i in range(grid_height):
        for j in range(grid_width):
            # Get tile coordinates
            y_start = i * tile_size
            y_end = min(y_start + tile_size, vis_frame.shape[0])
            x_start = j * tile_size
            x_end = min(x_start + tile_size, vis_frame.shape[1])
            
            # Get classification score for this tile
            score = classifications[i][j]
            
            # Calculate brightness factor based on threshold
            if score < threshold:
                # Reduce brightness for tiles below threshold
                brightness_factor = 0.3 + (score / threshold) * 0.4  # Range: 0.3 to 0.7
            else:
                # Keep normal brightness for tiles above threshold
                brightness_factor = 1.0
            
            # Apply brightness adjustment
            vis_frame[y_start:y_end, x_start:x_end] *= brightness_factor
    
    # Clip values to valid range and convert back to uint8
    vis_frame = np.clip(vis_frame, 0, 255).astype(np.uint8)
    
    return vis_frame


def create_overlay_frame(frame: np.ndarray, classifications: list[list[float]], 
                        tile_size: int, threshold: float) -> np.ndarray:
    """
    Create an overlay frame showing tile boundaries and classification scores.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        classifications (list[list[float]]): 2D grid of classification scores
        tile_size (int): Size of tiles used for classification
        threshold (float): Threshold value for visualization
        
    Returns:
        np.ndarray: Overlay frame with tile boundaries and scores
    """
    # Create a copy of the frame for overlay
    overlay_frame = frame.copy()
    
    # Get grid dimensions
    grid_height = len(classifications)
    grid_width = len(classifications[0]) if grid_height > 0 else 0
    
    # Calculate frame dimensions after padding
    vis_height = grid_height * tile_size
    vis_width = grid_width * tile_size
    
    # Ensure frame is large enough (handle padding)
    if frame.shape[0] < vis_height or frame.shape[1] < vis_width:
        # Pad frame if necessary
        pad_height = max(0, vis_height - frame.shape[0])
        pad_width = max(0, vis_width - frame.shape[1])
        overlay_frame = np.pad(overlay_frame, ((0, pad_height), (0, pad_width), (0, 0)), mode='constant')
    
    # Draw tile boundaries and add score text
    for i in range(grid_height):
        for j in range(grid_width):
            # Get tile coordinates
            y_start = i * tile_size
            y_end = min(y_start + tile_size, overlay_frame.shape[0])
            x_start = j * tile_size
            x_end = min(x_start + tile_size, overlay_frame.shape[1])
            
            # Get classification score for this tile
            score = classifications[i][j]
            
            # Determine color based on threshold
            if score < threshold:
                color = (0, 0, 255)  # Red for below threshold
            else:
                color = (0, 255, 0)  # Green for above threshold
            
            # Draw tile boundary
            cv2.rectangle(overlay_frame, (x_start, y_start), (x_end, y_end), color, 2)
            
            # Add score text (scaled for readability)
            score_text = f"{score:.2f}"
            font_scale = min(tile_size / 50.0, 0.8)  # Scale font based on tile size
            font_thickness = max(1, int(tile_size / 32))
            
            # Calculate text position (centered in tile)
            text_size = cv2.getTextSize(score_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]
            text_x = x_start + (tile_size - text_size[0]) // 2
            text_y = y_start + (tile_size + text_size[1]) // 2
            
            # Draw text with background for better visibility
            cv2.putText(overlay_frame, score_text, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness + 1)
            cv2.putText(overlay_frame, score_text, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, font_thickness)
    
    return overlay_frame


def save_visualization_frames(video_path: str, results: list, tile_size: int, 
                            threshold: float, output_dir: str):
    """
    Save visualization frames for a video as a video file.
    
    Args:
        video_path (str): Path to the input video file
        results (list): list of classification results from load_classification_results
        tile_size (int): Tile size used for classification
        threshold (float): Threshold value for visualization
        output_dir (str): Directory to save visualization video
        
    Raises:
        ValueError: If the number of video frames doesn't match the length of results
    """
    print(f"Creating visualizations for video: {video_path}")
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return
    
    # Get actual video frame count and properties
    video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    results_frame_count = len(results)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # Validate that video frame count matches results length
    if video_frame_count != results_frame_count:
        cap.release()
        raise ValueError(
            f"Frame count mismatch: Video has {video_frame_count} frames, "
            f"but results contain {results_frame_count} frames. "
            f"This suggests the classification results don't match the video file."
        )
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create video writer for brightness visualization
    brightness_video_path = os.path.join(output_dir, 'visualization.mp4')
    # Use MP4V codec for compatibility
    fourcc = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')
    brightness_writer = cv2.VideoWriter(brightness_video_path, fourcc, fps, (width, height))
    
    if not brightness_writer.isOpened():
        print(f"Error: Could not create video writer for {brightness_video_path}")
        cap.release()
        return
    
    print(f"Creating visualization video with {video_frame_count} frames at {fps} FPS")
    
    # Process all frames
    for frame_idx in tqdm(range(video_frame_count), desc="Creating visualization video"):
        # Get frame from video
        ret, frame = cap.read()
        if not ret:
            break
        
        # Get classification results for this frame
        frame_result = results[frame_idx]
        classifications = frame_result['tile_classifications']
        
        # Create brightness visualization frame
        brightness_frame = create_visualization_frame(frame, classifications, tile_size, threshold)
        
        # Write frame to video
        brightness_writer.write(brightness_frame)
    
    # Release resources
    cap.release()
    brightness_writer.release()
    
    print(f"Saved visualization video to: {brightness_video_path}")


def create_summary_visualization(results: list, tile_size: int, threshold: float, 
                               output_dir: str):
    """
    Create a summary visualization showing classification statistics.
    
    Args:
        results (list): list of classification results
        tile_size (int): Tile size used for classification
        threshold (float): Threshold value for visualization
        output_dir (str): Directory to save summary visualization
    """
    print("Creating summary visualization...")
    
    # Collect all scores
    all_scores = []
    for result in results:
        classifications = result['tile_classifications']
        for row in classifications:
            all_scores.extend(row)
    
    all_scores = np.array(all_scores)
    
    # Create summary plots
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # Histogram of all scores
    ax1.hist(all_scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.axvline(x=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
    ax1.set_xlabel('Classification Score')
    ax1.set_ylabel('Frequency')
    ax1.set_title(f'Distribution of Classification Scores (Tile Size: {tile_size})')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Box plot
    ax2.boxplot(all_scores)
    ax2.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
    ax2.set_ylabel('Classification Score')
    ax2.set_title(f'Box Plot of Classification Scores (Tile Size: {tile_size})')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Frame-by-frame average scores
    frame_avg_scores = []
    for result in results:
        classifications = result['tile_classifications']
        frame_scores = []
        for row in classifications:
            frame_scores.extend(row)
        frame_avg_scores.append(np.mean(frame_scores))
    
    ax3.plot(frame_avg_scores, color='blue', linewidth=2)
    ax3.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
    ax3.set_xlabel('Frame Index')
    ax3.set_ylabel('Average Classification Score')
    ax3.set_title(f'Frame-by-Frame Average Scores (Tile Size: {tile_size})')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Statistics table
    stats_text = f"""
    Total Tiles: {len(all_scores):,}
    Mean Score: {np.mean(all_scores):.4f}
    Median Score: {np.median(all_scores):.4f}
    Std Dev: {np.std(all_scores):.4f}
    Min Score: {np.min(all_scores):.4f}
    Max Score: {np.max(all_scores):.4f}
    
    Above Threshold: {np.sum(all_scores >= threshold):,} ({(np.sum(all_scores >= threshold) / len(all_scores) * 100):.1f}%)
    Below Threshold: {np.sum(all_scores < threshold):,} ({(np.sum(all_scores < threshold) / len(all_scores) * 100):.1f}%)
    """
    
    ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.set_title(f'Statistics Summary (Tile Size: {tile_size})')
    ax4.axis('off')
    
    plt.tight_layout()
    
    # Save summary plot
    summary_path = os.path.join(output_dir, f'summary_tile{tile_size}.png')
    plt.savefig(summary_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved summary visualization to: {summary_path}")


def main(args):
    """
    Main function that orchestrates the video tile classification visualization process.
    
    This function serves as the entry point for the script. It: 1. Validates the dataset directory exists
    2. Iterates through all videos in the dataset directory
    3. For each video, loads the classification results for the specified tile size(s)
    4. Creates visualizations showing tile classifications and brightness adjustments
    5. If --statistics flag is set, compares results with groundtruth and generates statistics
    
    Args:
        args (argparse.Namespace): Parsed command line arguments containing:
            - dataset (str): Name of the dataset to process
            - tile_size (str): Tile size to use for classification ('32', '64', '128', or 'all')
            - threshold (float): Threshold value for visualization (0.0 to 1.0)
            - groundtruth (bool): Whether to use groundtruth scores (score_correct.jsonl) instead of model scores (score.jsonl)
            - statistics (bool): Whether to compare classification results with groundtruth and generate statistics
            
         Note:
         - The script expects classification results from 020_exec_classify.py in:
           {CACHE_DIR}/{dataset}/{video_file}/relevancy/score/proxy_{tile_size}/
         - When groundtruth=True, looks for score_correct.jsonl files
         - When groundtruth=False, looks for score.jsonl files
         - Videos are read from {DATA_DIR}/{dataset}/
         - Visualizations are saved to {CACHE_DIR}/{dataset}/{video_file}/relevancy/proxy_{tile_size}/
         - The script creates a video file (visualization.mp4) showing brightness-adjusted frames
         - Summary statistics and plots are also generated
         - When --statistics is set, groundtruth comparison visualizations are generated instead of video output
         - When --statistics is set, the --groundtruth flag is automatically ignored (always uses model predictions)
    """
    dataset_dir = os.path.join(DATA_DIR, args.dataset)
    
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Dataset directory {dataset_dir} does not exist")
    
    # Validate threshold
    if not 0.0 <= args.threshold <= 1.0:
        raise ValueError("Threshold must be between 0.0 and 1.0")
    
    # Determine which tile sizes to process
    if args.tile_size == 'all':
        tile_sizes_to_process = TILE_SIZES
        print(f"Processing all tile sizes: {tile_sizes_to_process}")
    else:
        tile_sizes_to_process = [int(args.tile_size)]
        print(f"Processing tile size: {tile_sizes_to_process[0]}")
    
    print(f"Using threshold: {args.threshold}")
    
    if args.statistics:
        print("Running in statistics mode - comparing classification results with groundtruth")
        print("Note: --groundtruth flag is ignored in statistics mode (using model predictions)")
        use_model_scores = False
    else:
        use_model_scores = args.groundtruth
    
    # Get all video files from the dataset directory
    video_files = [f for f in os.listdir(dataset_dir) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]
    
    if not video_files:
        print(f"No video files found in {dataset_dir}")
        return
    
    print(f"Found {len(video_files)} video files to process")
    
    # Process each video file
    for video_file in sorted(video_files):
        video_file_path = os.path.join(dataset_dir, video_file)
        
        print(f"\nProcessing video file: {video_file}")
        
        # Process each tile size for this video
        for tile_size in tile_sizes_to_process:
            print(f"Processing tile size: {tile_size}")
            
            try:
                # Load classification results (model predictions for statistics mode)
                results = load_classification_results(CACHE_DIR, args.dataset, video_file, tile_size, use_model_scores)
                
                if args.statistics:
                    # Load groundtruth detections for comparison
                    try:
                        groundtruth_detections = load_groundtruth_detections(CACHE_DIR, args.dataset, video_file)
                        
                        # Create output directory for statistics visualizations
                        stats_output_dir = os.path.join(CACHE_DIR, args.dataset, video_file, 'relevancy', f'proxy_{tile_size}', 'statistics')
                        
                        # Create statistics visualizations
                        create_statistics_visualizations(
                            video_file, results, groundtruth_detections, 
                            tile_size, args.threshold, stats_output_dir
                        )
                        
                        print(f"Completed statistics analysis for tile size {tile_size}")
                        
                    except FileNotFoundError as e:
                        print(f"Warning: Could not load groundtruth for statistics: {e}")
                        print(f"Skipping statistics mode for tile size {tile_size}")
                        continue
                        
                else:
                    # Create output directory for visualizations
                    vis_output_dir = os.path.join(CACHE_DIR, args.dataset, video_file, 'relevancy', f'proxy_{tile_size}')
                    
                    # Create visualizations
                    save_visualization_frames(video_file_path, results, tile_size, args.threshold, vis_output_dir)
                    
                    # Create summary visualization
                    create_summary_visualization(results, tile_size, args.threshold, vis_output_dir)
                    
                    print(f"Completed visualizations for tile size {tile_size}")
                
            except FileNotFoundError as e:
                print(f"Warning: {e}, skipping tile size {tile_size} for video {video_file}")
                raise e
            except Exception as e:
                print(f"Error processing tile size {tile_size} for video {video_file}: {e}")
                raise e


if __name__ == '__main__':
    main(parse_args())
