#!/usr/local/bin/python

import argparse
import json
import os
import cv2
import numpy as np
from tqdm import tqdm
import multiprocessing as mp

CACHE_DIR = '/polyis-cache'
DATA_DIR = '/polyis-data/video-datasets-low'
TILE_SIZES = [64]

# Define 10 distinct colors for track visualization (BGR format for OpenCV)
TRACK_COLORS = [
    (255, 0, 0),    # Blue
    (0, 255, 0),    # Green
    (0, 0, 255),    # Red
    (255, 255, 0),  # Cyan
    (255, 0, 255),  # Magenta
    (0, 255, 255),  # Yellow
    (128, 0, 255),  # Purple
    (255, 128, 0),  # Orange
    (0, 128, 255),  # Light Blue
    (255, 0, 128),  # Pink
]


def parse_args():
    """
    Parse command line arguments for the script.
    
    Returns:
        argparse.Namespace: Parsed command line arguments containing:
            - dataset (str): Dataset name to process (default: 'b3d')
            - tile_size (str): Tile size to use for visualization (choices: '64', '128', 'all')
            - speed_up (int): Speed up factor for visualization (default: 4)
    """
    parser = argparse.ArgumentParser(description='Visualize tracking results from 060_exec_track.py on original videos')
    parser.add_argument('--dataset', required=False,
                        default='b3d',
                        help='Dataset name')
    parser.add_argument('--tile_size', type=str, choices=['64', '128', 'all'], default='all',
                        help='Tile size to use for visualization (or "all" for all tile sizes)')
    parser.add_argument('--speed_up', type=int, default=4,
                        help='Speed up factor for visualization (process every Nth frame)')
    return parser.parse_args()


def load_tracking_results(cache_dir: str, dataset: str, video_file: str, tile_size: int) -> dict[int, list[list[float]]]:
    """
    Load tracking results from the JSONL file generated by 060_exec_track.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tile_size (int): Tile size used for tracking
        
    Returns:
        dict[int, list[list[float]]]: dictionary mapping frame indices to lists of tracks
        
    Raises:
        FileNotFoundError: If no tracking results file is found
    """
    tracking_path = os.path.join(cache_dir, dataset, video_file, 'uncompressed_tracking',
                                f'proxy_{tile_size}', 'tracking.jsonl')
    
    if not os.path.exists(tracking_path):
        raise FileNotFoundError(f"Tracking results not found: {tracking_path}")
    
    print(f"Loading tracking results from: {tracking_path}")
    
    frame_tracks = {}
    with open(tracking_path, 'r') as f:
        for line in f:
            if line.strip():
                frame_data = json.loads(line)
                frame_idx = frame_data['frame_idx']
                tracks = frame_data['tracks']
                frame_tracks[frame_idx] = tracks
    
    print(f"Loaded tracking results for {len(frame_tracks)} frames")
    return frame_tracks


def get_track_color(track_id: int) -> tuple[int, int, int]:
    """
    Get a color for a track ID by cycling through the predefined colors.
    
    Args:
        track_id (int): Track ID
        
    Returns:
        tuple[int, int, int]: BGR color tuple
    """
    color_index = track_id % len(TRACK_COLORS)
    return TRACK_COLORS[color_index]


def create_visualization_frame(frame: np.ndarray, tracks: list[list[float]], 
                             frame_idx: int, trajectory_history: dict[int, list[tuple[int, int, int]]], 
                             speed_up: int) -> np.ndarray | None:
    """
    Create a visualization frame by drawing bounding boxes and trajectories for all tracks.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        tracks (list[list[float]]): list of tracks for this frame
        frame_idx (int): Frame index for logging
        trajectory_history (dict[int, list[tuple[int, int, int]]]): History of track centers with frame timestamps
        speed_up (int): Speed up factor (process every Nth frame)
        
    Returns:
        np.ndarray | None: Frame with bounding boxes and trajectories drawn, or None if frame should be skipped
    """
    # First loop: Update trajectory history for all tracks
    for track in tracks:
        if len(track) >= 5:  # Ensure we have track_id, x1, y1, x2, y2
            track_id, x1, y1, x2, y2 = track[:5]
            track_id = int(track_id)
            
            # Calculate center of bounding box
            center_x = int((x1 + x2) // 2)
            center_y = int((y1 + y2) // 2)
            
            # Update trajectory history with frame timestamp
            if track_id not in trajectory_history:
                trajectory_history[track_id] = []
            trajectory_history[track_id].append((center_x, center_y, frame_idx))

    if frame_idx % speed_up != 0:
        return None
    
    # Create a copy of the frame for visualization
    vis_frame = frame.copy()
    
    # Second loop: Draw bounding boxes and labels for current tracks
    for track in tracks:
        if len(track) >= 5:  # Ensure we have track_id, x1, y1, x2, y2
            track_id, x1, y1, x2, y2 = track[:5]
            
            # Convert to integers for drawing
            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
            track_id = int(track_id)
            
            # Get color for this track
            color = get_track_color(track_id)
            
            # Draw bounding box
            cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, 2)
            
            # Draw track ID label
            label = f"ID: {track_id}"
            font_scale = 0.6
            font_thickness = 2
            
            # Calculate text size and position
            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 
                                                                 font_scale, font_thickness)
            
            # Position text above the bounding box
            text_x = x1
            text_y = max(y1 - 10, text_height + 5)
            
            # Draw text background for better visibility
            cv2.rectangle(vis_frame, (text_x - 2, text_y - text_height - 2), 
                         (text_x + text_width + 2, text_y + baseline + 2), 
                         color, -1)
            
            # Draw text
            cv2.putText(vis_frame, label, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness)
    
    # Draw all trajectories with gradual fading
    for track_id, trajectory in trajectory_history.items():
        if len(trajectory) > 1:
            color = get_track_color(track_id)
            
            # Calculate fade parameters
            max_fade_frames = 30  # Number of frames for complete fade after track ends
            current_time = frame_idx
            
            # Check if track is still active (within last 5 frames)
            track_is_active = trajectory and current_time - trajectory[-1][2] <= 5
            
            # Calculate fade alpha for the entire trajectory
            if track_is_active:
                # Track is active - full opacity
                alpha = 1.0
            else:
                # Track has ended - calculate fade based on time since last detection
                time_since_end = current_time - trajectory[-1][2]
                if time_since_end >= max_fade_frames:
                    alpha = 0.0  # Completely faded
                else:
                    alpha = 1.0 - (time_since_end / max_fade_frames)
            
            # Only draw if trajectory is still visible
            if alpha > 0.01:
                # Apply alpha to color for the entire trajectory
                line_color = tuple(int(c * alpha) for c in color)
                point_color = tuple(int(c * alpha) for c in color)
                
                # Draw trajectory lines
                for i in range(1, len(trajectory)):
                    prev_center = trajectory[i-1]
                    curr_center = trajectory[i]
                    
                    # Draw line
                    cv2.line(vis_frame, (prev_center[0], prev_center[1]), 
                             (curr_center[0], curr_center[1]), line_color, 2)
                    
                    # Draw trajectory points
                    point_radius = max(1, int(3 * alpha))
                    cv2.circle(vis_frame, (prev_center[0], prev_center[1]), point_radius, point_color, -1)
                
                # Draw final point
                final_center = trajectory[-1]
                cv2.circle(vis_frame, (final_center[0], final_center[1]), 3, point_color, -1)
    
    return vis_frame


def create_tracking_visualization(video_path: str, tracking_results: dict[int, list[list[float]]], 
                                 output_path: str, speed_up: int, process_id: int):
    """
    Create a visualization video showing tracking results overlaid on the original video.
    
    Args:
        video_path (str): Path to the input video file
        tracking_results (dict[int, list[list[float]]]): Tracking results from load_tracking_results
        output_path (str): Path where the output visualization video will be saved
        speed_up (int): Speed up factor for visualization
        process_id (int): Process ID for logging
    """
    print(f"Creating tracking visualization for video: {video_path}")
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return
    
    # Get video properties
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"Video info: {width}x{height}, {fps} FPS, {frame_count} frames")
    
    # Create output directory if it doesn't exist
    output_dir = os.path.dirname(output_path)
    os.makedirs(output_dir, exist_ok=True)
    
    # Create video writer
    fourcc = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')
    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    if not writer.isOpened():
        print(f"Error: Could not create video writer for {output_path}")
        cap.release()
        return
    
    print(f"Creating visualization video with {frame_count} frames at {fps} FPS")
    
    # Initialize trajectory history for all tracks with frame timestamps
    trajectory_history: dict[int, list[tuple[int, int, int]]] = {}  # track_id -> [(x, y, frame_idx), ...]
    
    # Initialize frame_idx for exception handling
    frame_idx = 0
    
    # Process each frame
    try:
        for frame_idx in tqdm(range(frame_count), desc=f"Process {process_id} - Creating visualization", position=process_id):
            # Read frame
            ret, frame = cap.read()
            if not ret:
                break
            
            # Get tracking results for this frame
            tracks = tracking_results.get(frame_idx, [])
            
            # Create visualization frame with trajectory history
            vis_frame = create_visualization_frame(frame, tracks, frame_idx, trajectory_history, speed_up)
            
            # Write frame to video
            if vis_frame is not None:
                writer.write(vis_frame)
    
    except KeyboardInterrupt:
        print(f"\nProcess {process_id}: KeyboardInterrupt detected. Stopping video writing...")
        print(f"Process {process_id}: Video writing stopped at frame {frame_idx}")
    except Exception as e:
        print(f"\nProcess {process_id}: Error during video processing: {e}")
    finally:
        # Release resources
        cap.release()
        writer.release()
        print(f"Process {process_id}: Resources released")
    
    print(f"Process {process_id}: Tracking visualization completed")


def process_video_visualization(video_file: str, tile_size: int, cache_dir: str, dataset: str, speed_up: int, process_id: int):
    """
    Process visualization for a single video file.
    
    Args:
        video_file (str): Name of the video file to process
        tile_size (int): Tile size used for tracking
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        speed_up (int): Speed up factor for visualization
        process_id (int): Process ID for logging
    """
    try:
        # Load tracking results
        tracking_results = load_tracking_results(cache_dir, dataset, video_file, tile_size)
        
        # Get path to original video
        video_path = os.path.join(DATA_DIR, dataset, video_file)
        
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Original video not found for {video_file}")
        
        # Create output path for visualization
        output_path = os.path.join(cache_dir, dataset, video_file, 'uncompressed_tracking',
                                  f'proxy_{tile_size}', 'visualization.mp4')
        
        # Create visualization
        create_tracking_visualization(video_path, tracking_results, output_path, speed_up, process_id)
        
        print(f"Completed visualization for video: {video_file} with tile size: {tile_size}")
        
    except Exception as e:
        print(f"Error processing video {video_file} with tile size {tile_size}: {e}")
        raise e


def main(args):
    """
    Main function that orchestrates the tracking visualization process.
    
    This function serves as the entry point for the script. It:
    1. Validates the dataset directory exists
    2. Finds all videos with tracking results from 060_exec_track.py for the specified tile size(s)
    3. Creates a process pool for parallel processing
    4. Creates visualizations for each video and saves results
    
    Args:
        args (argparse.Namespace): Parsed command line arguments
        
    Note:
        - The script expects tracking results from 060_exec_track.py in:
          {CACHE_DIR}/{dataset}/{video_file}/uncompressed_tracking/proxy_{tile_size}/tracking.jsonl
        - Original videos are read from {DATA_DIR}/{dataset}/
        - Visualization videos are saved to:
          {CACHE_DIR}/{dataset}/{video_file}/uncompressed_tracking/proxy_{tile_size}/visualization.mp4
        - Each track ID gets a unique color from a predefined palette
        - Processing is parallelized for improved performance
        - When tile_size is 'all', all available tile sizes are processed
    """
    print(f"Processing dataset: {args.dataset}")
    print(f"Tile size: {args.tile_size}")
    print(f"Speed up factor: {args.speed_up} (processing every {args.speed_up}th frame)")
    
    # Determine which tile sizes to process
    if args.tile_size == 'all':
        tile_sizes_to_process = TILE_SIZES
        print(f"Processing all available tile sizes: {tile_sizes_to_process}")
    else:
        tile_sizes_to_process = [int(args.tile_size)]
        print(f"Processing tile size: {tile_sizes_to_process[0]}")
    
    # Find all videos with tracking results
    dataset_cache_dir = os.path.join(CACHE_DIR, args.dataset)
    if not os.path.exists(dataset_cache_dir):
        raise FileNotFoundError(f"Dataset cache directory {dataset_cache_dir} does not exist")
    
    # Look for directories that contain tracking results
    video_tile_combinations = []
    for item in os.listdir(dataset_cache_dir):
        item_path = os.path.join(dataset_cache_dir, item)
        if os.path.isdir(item_path):
            for tile_size in tile_sizes_to_process:
                tracking_path = os.path.join(item_path, 'uncompressed_tracking', f'proxy_{tile_size}', 'tracking.jsonl')
                if os.path.exists(tracking_path):
                    video_tile_combinations.append((item, tile_size))
    
    if not video_tile_combinations:
        print(f"No videos with tracking results found in {dataset_cache_dir}")
        return
    
    print(f"Found {len(video_tile_combinations)} video-tile size combinations to process")
    
    # Determine number of processes to use
    num_processes = min(mp.cpu_count(), len(video_tile_combinations), 20)  # Cap at 20 processes
    print(f"Using {num_processes} processes for parallel processing")
    
    # Create a pool of workers
    print(f"Creating process pool with {num_processes} workers...")
    
    # Prepare arguments for each video-tile combination
    video_args = []
    for i, (video_file, tile_size) in enumerate(video_tile_combinations):
        process_id = i % num_processes  # Assign process ID in round-robin fashion
        video_args.append((video_file, tile_size, CACHE_DIR, args.dataset, args.speed_up, process_id))
        print(f"Prepared video: {video_file} with tile size: {tile_size} for process {process_id}")
    
    # Use process pool to execute video visualization
    with mp.Pool(processes=num_processes) as pool:
        print(f"Starting video visualization with {num_processes} parallel workers...")
        
        # Map the work to the pool
        results = pool.starmap(process_video_visualization, video_args)
        
        print("All videos visualized successfully!")


if __name__ == '__main__':
    main(parse_args())
