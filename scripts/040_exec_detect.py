#!/usr/local/bin/python

import argparse
import json
import os
import time

import cv2
import tqdm

import polyis.models.retinanet_b3d

CACHE_DIR = '/polyis-cache'


def format_time(**kwargs):
    return [{ 'op': op, 'time': time } for op, time in kwargs.items()]


def parse_args():
    """
    Parse command line arguments for the script.
    
    Returns:
        argparse.Namespace: Parsed command line arguments containing:
            - dataset (str): Dataset name to process (default: 'b3d')
            - tile_size (str): Tile size to use for detection (choices: '64', '128', 'all')
            - detector (str): Detector name to use (default: 'retina')
    """
    parser = argparse.ArgumentParser(description='Detect objects from packed images generated by 030_exec_pack.py')
    parser.add_argument('--dataset', required=False,
                        default='b3d',
                        help='Dataset name')
    parser.add_argument('--tile_size', type=str, choices=['64', '128', 'all'], default='all',
                        help='Tile size to use for detection (or "all" for all tile sizes)')
    parser.add_argument('--detector', required=False,
                        default='retina',
                        help='Detector name')
    return parser.parse_args()


def detect_retina_packed_images(dataset_dir: str, tile_size: int, dataset_name: str):
    """
    Detect objects from packed images using RetinaNet detector.
    
    Args:
        dataset_dir (str): Directory containing video files
        tile_size (int): Tile size used for packing
        dataset_name (str): Name of the dataset
    """
    detector = polyis.models.retinanet_b3d.get_detector(device='cuda:0')

    for video in os.listdir(dataset_dir):
        video_path = os.path.join(dataset_dir, video)
        if not os.path.isdir(video_path):
            continue

        print(f"Processing video {video_path}")

        # Check if packing directory exists for this tile size
        packing_dir = os.path.join(CACHE_DIR, dataset_name, video, 'packing', f'proxy_{tile_size}', 'images')
        if not os.path.exists(packing_dir):
            print(f"Packing directory not found for video {video} and tile size {tile_size}, skipping...")
            continue

        # Create output directory for detections
        detections_output_dir = os.path.join(CACHE_DIR, dataset_name, video, 'packed_detections', f'proxy_{tile_size}', 'detections')
        os.makedirs(detections_output_dir, exist_ok=True)

        # Get all packed image files
        image_files = [f for f in os.listdir(packing_dir) if f.endswith('.jpg')]
        
        if not image_files:
            print(f"No packed images found in {packing_dir}")
            continue

        print(f"Found {len(image_files)} packed images to process")

        for image_file in tqdm.tqdm(image_files, desc=f"Processing packed images for tile size {tile_size}"):
            image_path = os.path.join(packing_dir, image_file)
            
            # Read the packed image
            start_time = time.time_ns()
            frame = cv2.imread(image_path)
            if frame is None:
                print(f"Warning: Could not read image {image_path}, skipping...")
                continue
            end_time = time.time_ns()
            read_time = end_time - start_time

            # Detect objects in the frame
            start_time = time.time_ns()
            outputs = polyis.models.retinanet_b3d.detect(frame, detector)
            end_time = time.time_ns()
            detect_time = end_time - start_time

            # Extract bounding boxes (x1, y1, x2, y2 format)
            bounding_boxes = outputs[:, :4].tolist()

            # Save detection results
            output_file = os.path.join(detections_output_dir, f"{image_file}.jsonl")
            with open(output_file, 'w') as f:
                for bbox in bounding_boxes:
                    f.write(json.dumps(bbox) + '\n')

            print(f"Saved {len(bounding_boxes)} detections to {output_file}")


def main(args):
    """
    Main function that orchestrates the object detection process on packed images.
    
    This function serves as the entry point for the script. It:
    1. Validates the dataset directory exists
    2. Iterates through all videos in the dataset directory
    3. For each video, finds packed images for the specified tile size(s)
    4. Detects objects in each packed image using the specified detector
    5. Saves detection results as JSONL files with bounding box coordinates
    
    Args:
        args (argparse.Namespace): Parsed command line arguments containing:
            - dataset (str): Name of the dataset to process
            - tile_size (str): Tile size to use for detection ('64', '128', or 'all')
            - detector (str): Name of the detector to use
            
    Note:
        - The script expects packed images from 030_exec_pack.py in:
          {CACHE_DIR}/{dataset}/{video_file}/packing/proxy_{tile_size}/images/
        - Detection results are saved to:
          {CACHE_DIR}/{dataset}/{video_file}/packed_detections/proxy_{tile_size}/detections/{image_file}.jsonl
        - Each line in the output JSONL file contains one bounding box [x1, y1, x2, y2]
        - When tile_size is 'all', all available tile sizes are processed
        - If no packed images are found for a video/tile_size combination, that combination is skipped
    """
    dataset_dir = os.path.join(CACHE_DIR, args.dataset)
    
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Dataset directory {dataset_dir} does not exist")
    
    # Determine which tile sizes to process
    if args.tile_size == 'all':
        # Check what tile sizes are available by looking at existing packing directories
        available_tile_sizes = []
        for video in os.listdir(dataset_dir):
            video_path = os.path.join(dataset_dir, video)
            if not os.path.isdir(video_path):
                continue
            
            # Check what tile sizes have packing directories
            for tile_size in [64, 128]:
                packing_dir = os.path.join(CACHE_DIR, args.dataset, video, 'packing', f'proxy_{tile_size}', 'images')
                if os.path.exists(packing_dir):
                    available_tile_sizes.append(tile_size)
        
        # Remove duplicates and sort
        available_tile_sizes = sorted(list(set(available_tile_sizes)))
        if not available_tile_sizes:
            print("No packing directories found for any tile size")
            return
        
        tile_sizes_to_process = available_tile_sizes
        print(f"Processing all available tile sizes: {tile_sizes_to_process}")
    else:
        tile_sizes_to_process = [int(args.tile_size)]
        print(f"Processing tile size: {tile_sizes_to_process[0]}")
    
    # Get all video files from the dataset directory
    video_files = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]
    
    if not video_files:
        print(f"No video directories found in {dataset_dir}")
        return
    
    print(f"Found {len(video_files)} video directories to process")
    
    # Process each video file
    for video_file in sorted(video_files):
        video_file_path = os.path.join(dataset_dir, video_file)
        
        print(f"\nProcessing video file: {video_file}")
        
        # Process each tile size for this video
        for tile_size in tile_sizes_to_process:
            print(f"Processing tile size: {tile_size}")
            
            try:
                if args.detector == 'retina':
                    detect_retina_packed_images(dataset_dir, tile_size, args.dataset)
                else:
                    raise ValueError(f"Unknown detector: {args.detector}")
                
                print(f"Completed detection for tile size {tile_size}")
                
            except Exception as e:
                print(f"Error processing tile size {tile_size} for video {video_file}: {e}")
                continue


if __name__ == '__main__':
    main(parse_args())
