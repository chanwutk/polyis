#!/usr/local/bin/python

import argparse
import json
import os
import shutil
import time
import numpy as np
import cv2
import tqdm
from typing import Dict, List, Tuple, Any

CACHE_DIR = '/polyis-cache'
# TILE_SIZES = [32, 64, 128]
TILE_SIZES = [64]


def format_time(**kwargs):
    return [{ 'op': op, 'time': time } for op, time in kwargs.items()]


def parse_args():
    """
    Parse command line arguments for the script.
    
    Returns:
        argparse.Namespace: Parsed command line arguments containing:
            - dataset (str): Dataset name to process (default: 'b3d')
            - tile_size (str): Tile size to use for unpacking (choices: '64', '128', 'all')
    """
    parser = argparse.ArgumentParser(description='Unpack detection results from packed detections generated by 040_exec_detect.py')
    parser.add_argument('--dataset', required=False,
                        default='b3d',
                        help='Dataset name')
    parser.add_argument('--tile_size', type=str, choices=['64', '128', 'all'], default='all',
                        help='Tile size to use for unpacking (or "all" for all tile sizes)')
    return parser.parse_args()


def load_mapping_file(mapping_path: str) -> Dict[str, Any]:
    """
    Load mapping file that contains the index_map and det_info for unpacking.
    
    Args:
        mapping_path (str): Path to the mapping file
        
    Returns:
        Dict[str, Any]: Mapping information containing index_map, det_info, frame_range, etc.
        
    Raises:
        FileNotFoundError: If mapping file doesn't exist
        json.JSONDecodeError: If mapping file is invalid JSON
    """
    if not os.path.exists(mapping_path):
        raise FileNotFoundError(f"Mapping file not found: {mapping_path}")
    
    with open(mapping_path, 'r') as f:
        mapping_data = json.load(f)
    
    # Convert lists back to numpy arrays for index_map
    if 'index_map' in mapping_data:
        mapping_data['index_map'] = np.array(mapping_data['index_map'], dtype=np.int32)
    
    # Convert det_info keys back to tuples
    if 'det_info' in mapping_data:
        det_info = {}
        for k_str, v in mapping_data['det_info'].items():
            # Parse the string key back to tuple (frame_idx, group_id)
            k_str = k_str.strip('()')
            frame_idx, group_id = map(int, k_str.split(','))
            det_info[(frame_idx, group_id)] = v
        mapping_data['det_info'] = det_info
    
    return mapping_data


def load_detection_file(detection_path: str) -> List[List[float]]:
    """
    Load detection results from a JSONL file.
    
    Args:
        detection_path (str): Path to the detection file
        
    Returns:
        List[List[float]]: List of bounding boxes, each as [x1, y1, x2, y2]
        
    Raises:
        FileNotFoundError: If detection file doesn't exist
        json.JSONDecodeError: If detection file contains invalid JSON
    """
    if not os.path.exists(detection_path):
        raise FileNotFoundError(f"Detection file not found: {detection_path}")
    
    detections = []
    with open(detection_path, 'r') as f:
        for line in f:
            if line.strip():
                bbox = json.loads(line)
                detections.append(bbox)
    
    return detections


def unpack_detections(detections: List[List[float]], 
                     mapping_data: Dict[str, Any], 
                     tile_size: int) -> Dict[int, List[List[float]]]:
    """
    Unpack detections from packed coordinates back to original frame coordinates.
    
    Args:
        detections (List[List[float]]): List of bounding boxes in packed coordinates [x1, y1, x2, y2]
        mapping_data (Dict[str, Any]): Mapping information from the mapping file
        tile_size (int): Size of tiles used for packing
        
    Returns:
        Dict[int, List[List[float]]]: Dictionary mapping frame indices to lists of bounding boxes
                                     in original frame coordinates
    """
    index_map = mapping_data['index_map']
    det_info = mapping_data['det_info']
    frame_range = mapping_data['frame_range']
    
    # Initialize dictionary to store detections per frame
    frame_detections: Dict[int, List[List[float]]] = {}
    
    # Process each detection
    for det in detections:
        # Get the center point of the detection in packed coordinates
        center_x = (det[0] + det[2]) / 2.0
        center_y = (det[1] + det[3]) / 2.0
        
        # Convert to tile coordinates in the packed image
        tile_x = int(center_x // tile_size)
        tile_y = int(center_y // tile_size)
        
        # Ensure tile coordinates are within bounds
        if (tile_y < 0 or tile_y >= index_map.shape[0] or 
            tile_x < 0 or tile_x >= index_map.shape[1]):
            raise ValueError(f"Detection {det} is outside tile bounds")
        
        # Get the group ID and frame index for this tile
        group_id = int(index_map[tile_y, tile_x, 0])
        frame_idx = int(index_map[tile_y, tile_x, 1])
        
        # Skip if this tile has no group (group_id == 0)
        if group_id == 0:
            raise ValueError(f"Tile {tile_x}, {tile_y} has no group")
        
        # Get the offset information for this group
        if (frame_idx, group_id) not in det_info:
            raise ValueError(f"No mapping info for frame {frame_idx}, group {group_id}")
        
        (packed_y, packed_x), (original_offset_y, original_offset_x) = det_info[(frame_idx, group_id)]
        
        # Calculate the offset to convert from packed to original coordinates
        # The offset represents how much the tile was moved during packing
        offset_x = (original_offset_x - packed_x) * tile_size
        offset_y = (original_offset_y - packed_y) * tile_size
        
        # Convert detection back to original frame coordinates
        original_det = [
            det[0] + offset_x,  # x1
            det[1] + offset_y,  # y1
            det[2] + offset_x,  # x2
            det[3] + offset_y   # y2
        ]
        
        # Add to frame detections
        if frame_idx not in frame_detections:
            frame_detections[frame_idx] = []
        frame_detections[frame_idx].append(original_det)
    
    return frame_detections


def process_video_unpacking(video_file_path: str, tile_size: int, dataset_name: str):
    """
    Process a single video for unpacking detection results.
    
    Args:
        video_file_path (str): Path to the video file directory
        tile_size (int): Tile size used for packing
        dataset_name (str): Name of the dataset
    """
    print(f"Processing video {video_file_path} for unpacking")
    
    # Check if packed detections directory exists
    detections_dir = os.path.join(video_file_path, 'packed_detections', f'proxy_{tile_size}', 'detections')
    if not os.path.exists(detections_dir):
        raise FileNotFoundError(f"Packed detections directory not found: {detections_dir}")
    
    # Check if packing directory exists for mapping files
    packing_dir = os.path.join(video_file_path, 'packing', f'proxy_{tile_size}', 'images')
    if not os.path.exists(packing_dir):
        raise FileNotFoundError(f"Packing directory not found: {packing_dir}")
    
    # Create output directory for unpacked detections
    unpacked_output_dir = os.path.join(video_file_path, 'unpacked_detections', f'proxy_{tile_size}')
    if os.path.exists(unpacked_output_dir):
        # Remove the entire directory
        shutil.rmtree(unpacked_output_dir)
    os.makedirs(unpacked_output_dir, exist_ok=True)
    print(f"Saving unpacked detections to {unpacked_output_dir}")
    
    # Get all detection files
    detection_files = [f for f in os.listdir(detections_dir) if f.endswith('.jsonl')]
    
    if not detection_files:
        raise FileNotFoundError(f"No detection files found in {detections_dir}")
    
    print(f"Found {len(detection_files)} detection files to unpack")
    
    # Dictionary to store all frame detections
    all_frame_detections: Dict[int, List[List[float]]] = {}
    
    # Process each detection file
    for detection_file in tqdm.tqdm(detection_files, desc=f"Unpacking detections for tile size {tile_size}"):
        # Extract the image number from the filename (e.g., "img_00000001.jpg.jsonl" -> "00000001")
        img_num = detection_file.replace('img_', '').replace('.jpg.jsonl', '')
        
        # Construct paths
        detection_path = os.path.join(detections_dir, detection_file)
        mapping_path = os.path.join(packing_dir, f'mapping_{img_num}.json')
        
        # Load detection results
        detections = load_detection_file(detection_path)
        
        # Load corresponding mapping file
        mapping_data = load_mapping_file(mapping_path)
        
        # Unpack detections
        frame_detections = unpack_detections(detections, mapping_data, tile_size)
        
        # Merge with existing frame detections
        for frame_idx, bboxes in frame_detections.items():
            if frame_idx not in all_frame_detections:
                all_frame_detections[frame_idx] = []
            all_frame_detections[frame_idx].extend(bboxes)
    
    # Save unpacked detections organized by frame
    # Sort frames by index
    sorted_frames = sorted(all_frame_detections.keys())
    
    # Save each frame's detections
    for frame_idx in sorted_frames:
        bboxes = all_frame_detections[frame_idx]
        
        # Create frame-specific output file
        frame_output_file = os.path.join(unpacked_output_dir, f'frame_{frame_idx:08d}.jsonl')
        
        with open(frame_output_file, 'w') as f:
            for bbox in bboxes:
                f.write(json.dumps(bbox) + '\n')
    
    print(f"Saved unpacked detections for {len(sorted_frames)} frames")


def main(args):
    """
    Main function that orchestrates the detection unpacking process.
    
    This function serves as the entry point for the script. It:
    1. Validates the dataset directory exists
    2. Iterates through all videos in the dataset directory
    3. For each video, finds packed detection files for the specified tile size(s)
    4. Loads corresponding mapping files and unpacks detections back to original frame coordinates
    5. Saves unpacked detections organized by frame
    
    Args:
        args (argparse.Namespace): Parsed command line arguments containing:
            - dataset (str): Name of the dataset to process
            - tile_size (str): Tile size to use for unpacking ('64', '128', or 'all')
            
    Note:
        - The script expects packed detections from 040_exec_detect.py in:
          {CACHE_DIR}/{dataset}/{video_file}/packed_detections/proxy_{tile_size}/detections/
        - The script expects mapping files from 030_exec_pack.py in:
          {CACHE_DIR}/{dataset}/{video_file}/packing/proxy_{tile_size}/images/
        - Unpacked detections are saved to:
          {CACHE_DIR}/{dataset}/{video_file}/unpacked_detections/proxy_{tile_size}/frame_{frame_idx}.jsonl
        - Each line in the output JSONL file contains one bounding box [x1, y1, x2, y2] in original frame coordinates
        - When tile_size is 'all', all available tile sizes are processed
        - If no packed detections are found for a video/tile_size combination, that combination is skipped
    """
    dataset_dir = os.path.join(CACHE_DIR, args.dataset)
    
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Dataset directory {dataset_dir} does not exist")
    
    # Determine which tile sizes to process
    if args.tile_size == 'all':
        tile_sizes_to_process = TILE_SIZES
        print(f"Processing all available tile sizes: {tile_sizes_to_process}")
    else:
        tile_sizes_to_process = [int(args.tile_size)]
        print(f"Processing tile size: {tile_sizes_to_process[0]}")
    
    # Get all video files from the dataset directory
    video_files = [f for f in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, f))]
    
    if not video_files:
        print(f"No video directories found in {dataset_dir}")
        return
    
    print(f"Found {len(video_files)} video directories to process")
    
    # Process each video file
    for video_file in sorted(video_files):
        video_file_path = os.path.join(dataset_dir, video_file)
        
        print(f"\nProcessing video file: {video_file}")
        
        # Process each tile size for this video
        for tile_size in tile_sizes_to_process:
            print(f"Processing tile size: {tile_size}")
            
            try:
                process_video_unpacking(video_file_path, tile_size, args.dataset)
                print(f"Completed unpacking for tile size {tile_size}")
                
            except Exception as e:
                print(f"Error processing tile size {tile_size} for video {video_file}: {e}")
                continue


if __name__ == '__main__':
    main(parse_args())
