#!/usr/local/bin/python

import argparse
import json
import os
import cv2
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt


DATA_DIR = '/polyis-data/video-datasets-low'
CACHE_DIR = '/polyis-cache'
TILE_SIZES = [32, 64, 128]


def parse_args():
    """
    Parse command line arguments for the script.
    
    Returns:
        argparse.Namespace: Parsed command line arguments containing:
            - dataset (str): Dataset name to process (default: 'b3d')
            - tile_size (int | str): Tile size to use for classification (choices: 32, 64, 128, 'all')
            - threshold (float): Threshold for classification visualization (default: 0.5)
    """
    parser = argparse.ArgumentParser(description='Visualize video tile classification results')
    parser.add_argument('--dataset', required=False,
                        default='b3d',
                        help='Dataset name')
    parser.add_argument('--tile_size', type=str, choices=['32', '64', '128', 'all'], default='all',
                        help='Tile size to use for classification (or "all" for all tile sizes)')
    parser.add_argument('--threshold', type=float, default=0.5,
                        help='Threshold for classification visualization (0.0 to 1.0)')
    return parser.parse_args()


def load_classification_results(cache_dir: str, dataset: str, video_file: str, tile_size: int) -> list:
    """
    Load classification results from the JSONL file generated by 020_exec_classify.py.
    
    Args:
        cache_dir (str): Cache directory path
        dataset (str): Dataset name
        video_file (str): Video file name
        tile_size (int): Tile size used for classification
        
    Returns:
        list: List of frame classification results, each containing frame data and classifications
        
    Raises:
        FileNotFoundError: If no classification results file is found
    """
    # Look for the classification results file
    score_dir = os.path.join(cache_dir, dataset, video_file, 'relevancy', 'score', f'proxy_{tile_size}')
    
    # Find the actual results file (it should be named with the frame count)
    if not os.path.exists(score_dir):
        raise FileNotFoundError(f"Score directory not found: {score_dir}")
    
    # Look for the JSONL file in the score directory
    jsonl_files = [f for f in os.listdir(score_dir) if f.endswith('.jsonl')]
    if not jsonl_files:
        raise FileNotFoundError(f"No classification results found in {score_dir}")
    
    # Use the first JSONL file found (should be the only one)
    results_file = os.path.join(score_dir, jsonl_files[0])
    
    print(f"Loading classification results from: {results_file}")
    
    results = []
    with open(results_file, 'r') as f:
        for line in f:
            if line.strip():
                results.append(json.loads(line))
    
    print(f"Loaded {len(results)} frame classifications")
    return results


def create_visualization_frame(frame: np.ndarray, classifications: list[list[float]], 
                              tile_size: int, threshold: float) -> np.ndarray:
    """
    Create a visualization frame by adjusting tile brightness based on classification scores.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        classifications (list[list[float]]): 2D grid of classification scores
        tile_size (int): Size of tiles used for classification
        threshold (float): Threshold value for visualization
        
    Returns:
        np.ndarray: Visualization frame with adjusted tile brightness
    """
    # Create a copy of the frame for visualization
    vis_frame = frame.copy().astype(np.float32)
    
    # Get grid dimensions
    grid_height = len(classifications)
    grid_width = len(classifications[0]) if grid_height > 0 else 0
    
    # Calculate frame dimensions after padding
    vis_height = grid_height * tile_size
    vis_width = grid_width * tile_size
    
    # Ensure frame is large enough (handle padding)
    if frame.shape[0] < vis_height or frame.shape[1] < vis_width:
        # Pad frame if necessary
        pad_height = max(0, vis_height - frame.shape[0])
        pad_width = max(0, vis_width - frame.shape[1])
        vis_frame = np.pad(vis_frame, ((0, pad_height), (0, pad_width), (0, 0)), mode='constant')
    
    # Apply brightness adjustments to each tile
    for i in range(grid_height):
        for j in range(grid_width):
            # Get tile coordinates
            y_start = i * tile_size
            y_end = min(y_start + tile_size, vis_frame.shape[0])
            x_start = j * tile_size
            x_end = min(x_start + tile_size, vis_frame.shape[1])
            
            # Get classification score for this tile
            score = classifications[i][j]
            
            # Calculate brightness factor based on threshold
            if score < threshold:
                # Reduce brightness for tiles below threshold
                brightness_factor = 0.3 + (score / threshold) * 0.4  # Range: 0.3 to 0.7
            else:
                # Keep normal brightness for tiles above threshold
                brightness_factor = 1.0
            
            # Apply brightness adjustment
            vis_frame[y_start:y_end, x_start:x_end] *= brightness_factor
    
    # Clip values to valid range and convert back to uint8
    vis_frame = np.clip(vis_frame, 0, 255).astype(np.uint8)
    
    return vis_frame


def create_overlay_frame(frame: np.ndarray, classifications: list[list[float]], 
                        tile_size: int, threshold: float) -> np.ndarray:
    """
    Create an overlay frame showing tile boundaries and classification scores.
    
    Args:
        frame (np.ndarray): Original video frame (H, W, 3)
        classifications (list[list[float]]): 2D grid of classification scores
        tile_size (int): Size of tiles used for classification
        threshold (float): Threshold value for visualization
        
    Returns:
        np.ndarray: Overlay frame with tile boundaries and scores
    """
    # Create a copy of the frame for overlay
    overlay_frame = frame.copy()
    
    # Get grid dimensions
    grid_height = len(classifications)
    grid_width = len(classifications[0]) if grid_height > 0 else 0
    
    # Calculate frame dimensions after padding
    vis_height = grid_height * tile_size
    vis_width = grid_width * tile_size
    
    # Ensure frame is large enough (handle padding)
    if frame.shape[0] < vis_height or frame.shape[1] < vis_width:
        # Pad frame if necessary
        pad_height = max(0, vis_height - frame.shape[0])
        pad_width = max(0, vis_width - frame.shape[1])
        overlay_frame = np.pad(overlay_frame, ((0, pad_height), (0, pad_width), (0, 0)), mode='constant')
    
    # Draw tile boundaries and add score text
    for i in range(grid_height):
        for j in range(grid_width):
            # Get tile coordinates
            y_start = i * tile_size
            y_end = min(y_start + tile_size, overlay_frame.shape[0])
            x_start = j * tile_size
            x_end = min(x_start + tile_size, overlay_frame.shape[1])
            
            # Get classification score for this tile
            score = classifications[i][j]
            
            # Determine color based on threshold
            if score < threshold:
                color = (0, 0, 255)  # Red for below threshold
            else:
                color = (0, 255, 0)  # Green for above threshold
            
            # Draw tile boundary
            cv2.rectangle(overlay_frame, (x_start, y_start), (x_end, y_end), color, 2)
            
            # Add score text (scaled for readability)
            score_text = f"{score:.2f}"
            font_scale = min(tile_size / 50.0, 0.8)  # Scale font based on tile size
            font_thickness = max(1, int(tile_size / 32))
            
            # Calculate text position (centered in tile)
            text_size = cv2.getTextSize(score_text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)[0]
            text_x = x_start + (tile_size - text_size[0]) // 2
            text_y = y_start + (tile_size + text_size[1]) // 2
            
            # Draw text with background for better visibility
            cv2.putText(overlay_frame, score_text, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), font_thickness + 1)
            cv2.putText(overlay_frame, score_text, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, font_thickness)
    
    return overlay_frame


def save_visualization_frames(video_path: str, results: list, tile_size: int, 
                            threshold: float, output_dir: str):
    """
    Save visualization frames for a video as a video file.
    
    Args:
        video_path (str): Path to the input video file
        results (list): List of classification results from load_classification_results
        tile_size (int): Tile size used for classification
        threshold (float): Threshold value for visualization
        output_dir (str): Directory to save visualization video
        
    Raises:
        ValueError: If the number of video frames doesn't match the length of results
    """
    print(f"Creating visualizations for video: {video_path}")
    
    # Open video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Could not open video {video_path}")
        return
    
    # Get actual video frame count and properties
    video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    results_frame_count = len(results)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # Validate that video frame count matches results length
    if video_frame_count != results_frame_count:
        cap.release()
        raise ValueError(
            f"Frame count mismatch: Video has {video_frame_count} frames, "
            f"but results contain {results_frame_count} frames. "
            f"This suggests the classification results don't match the video file."
        )
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Create video writer for brightness visualization
    brightness_video_path = os.path.join(output_dir, 'visualization.mp4')
    # Use MP4V codec for compatibility
    fourcc = cv2.VideoWriter.fourcc('m', 'p', '4', 'v')
    brightness_writer = cv2.VideoWriter(brightness_video_path, fourcc, fps, (width, height))
    
    if not brightness_writer.isOpened():
        print(f"Error: Could not create video writer for {brightness_video_path}")
        cap.release()
        return
    
    print(f"Creating visualization video with {video_frame_count} frames at {fps} FPS")
    
    # Process all frames
    for frame_idx in tqdm(range(video_frame_count), desc="Creating visualization video"):
        # Get frame from video
        ret, frame = cap.read()
        if not ret:
            break
        
        # Get classification results for this frame
        frame_result = results[frame_idx]
        classifications = frame_result['tile_classifications']
        
        # Create brightness visualization frame
        brightness_frame = create_visualization_frame(frame, classifications, tile_size, threshold)
        
        # Write frame to video
        brightness_writer.write(brightness_frame)
    
    # Release resources
    cap.release()
    brightness_writer.release()
    
    print(f"Saved visualization video to: {brightness_video_path}")


def create_summary_visualization(results: list, tile_size: int, threshold: float, 
                               output_dir: str):
    """
    Create a summary visualization showing classification statistics.
    
    Args:
        results (list): List of classification results
        tile_size (int): Tile size used for classification
        threshold (float): Threshold value for visualization
        output_dir (str): Directory to save summary visualization
    """
    print("Creating summary visualization...")
    
    # Collect all scores
    all_scores = []
    for result in results:
        classifications = result['tile_classifications']
        for row in classifications:
            all_scores.extend(row)
    
    all_scores = np.array(all_scores)
    
    # Create summary plots
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # Histogram of all scores
    ax1.hist(all_scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.axvline(x=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
    ax1.set_xlabel('Classification Score')
    ax1.set_ylabel('Frequency')
    ax1.set_title(f'Distribution of Classification Scores (Tile Size: {tile_size})')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Box plot
    ax2.boxplot(all_scores)
    ax2.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
    ax2.set_ylabel('Classification Score')
    ax2.set_title(f'Box Plot of Classification Scores (Tile Size: {tile_size})')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Frame-by-frame average scores
    frame_avg_scores = []
    for result in results:
        classifications = result['tile_classifications']
        frame_scores = []
        for row in classifications:
            frame_scores.extend(row)
        frame_avg_scores.append(np.mean(frame_scores))
    
    ax3.plot(frame_avg_scores, color='blue', linewidth=2)
    ax3.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')
    ax3.set_xlabel('Frame Index')
    ax3.set_ylabel('Average Classification Score')
    ax3.set_title(f'Frame-by-Frame Average Scores (Tile Size: {tile_size})')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Statistics table
    stats_text = f"""
    Total Tiles: {len(all_scores):,}
    Mean Score: {np.mean(all_scores):.4f}
    Median Score: {np.median(all_scores):.4f}
    Std Dev: {np.std(all_scores):.4f}
    Min Score: {np.min(all_scores):.4f}
    Max Score: {np.max(all_scores):.4f}
    
    Above Threshold: {np.sum(all_scores >= threshold):,} ({(np.sum(all_scores >= threshold) / len(all_scores) * 100):.1f}%)
    Below Threshold: {np.sum(all_scores < threshold):,} ({(np.sum(all_scores < threshold) / len(all_scores) * 100):.1f}%)
    """
    
    ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes, fontsize=10,
             verticalalignment='top', fontfamily='monospace',
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.set_title(f'Statistics Summary (Tile Size: {tile_size})')
    ax4.axis('off')
    
    plt.tight_layout()
    
    # Save summary plot
    summary_path = os.path.join(output_dir, f'summary_tile{tile_size}.png')
    plt.savefig(summary_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved summary visualization to: {summary_path}")


def main(args):
    """
    Main function that orchestrates the video tile classification visualization process.
    
    This function serves as the entry point for the script. It: 1. Validates the dataset directory exists
    2. Iterates through all videos in the dataset directory
    3. For each video, loads the classification results for the specified tile size(s)
    4. Creates visualizations showing tile classifications and brightness adjustments
    
    Args:
        args (argparse.Namespace): Parsed command line arguments containing:
            - dataset (str): Name of the dataset to process
            - tile_size (str): Tile size to use for classification ('32', '64', '128', or 'all')
            - threshold (float): Threshold value for visualization (0.0 to 1.0)
            
         Note:
         - The script expects classification results from 020_exec_classify.py in:
           {CACHE_DIR}/{dataset}/{video_file}/relevancy/score/proxy_{tile_size}/
         - Videos are read from {DATA_DIR}/{dataset}/
         - Visualizations are saved to {CACHE_DIR}/{dataset}/{video_file}/relevancy/proxy_{tile_size}/
         - The script creates a video file (visualization.mp4) showing brightness-adjusted frames
         - Summary statistics and plots are also generated
    """
    dataset_dir = os.path.join(DATA_DIR, args.dataset)
    
    if not os.path.exists(dataset_dir):
        raise FileNotFoundError(f"Dataset directory {dataset_dir} does not exist")
    
    # Validate threshold
    if not 0.0 <= args.threshold <= 1.0:
        raise ValueError("Threshold must be between 0.0 and 1.0")
    
    # Determine which tile sizes to process
    if args.tile_size == 'all':
        tile_sizes_to_process = TILE_SIZES
        print(f"Processing all tile sizes: {tile_sizes_to_process}")
    else:
        tile_sizes_to_process = [int(args.tile_size)]
        print(f"Processing tile size: {tile_sizes_to_process[0]}")
    
    print(f"Using threshold: {args.threshold}")
    
    # Get all video files from the dataset directory
    video_files = [f for f in os.listdir(dataset_dir) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]
    
    if not video_files:
        print(f"No video files found in {dataset_dir}")
        return
    
    print(f"Found {len(video_files)} video files to visualize")
    
    # Process each video file
    for video_file in video_files:
        video_file_path = os.path.join(dataset_dir, video_file)
        
        print(f"\nProcessing video file: {video_file}")
        
        # Process each tile size for this video
        for tile_size in tile_sizes_to_process:
            print(f"Processing tile size: {tile_size}")
            
            try:
                # Load classification results
                results = load_classification_results(CACHE_DIR, args.dataset, video_file, tile_size)
                
                # Create output directory for visualizations
                vis_output_dir = os.path.join(CACHE_DIR, args.dataset, video_file, 'relevancy', f'proxy_{tile_size}')
                
                # Create visualizations
                save_visualization_frames(video_file_path, results, tile_size, args.threshold, vis_output_dir)
                
                # Create summary visualization
                create_summary_visualization(results, tile_size, args.threshold, vis_output_dir)
                
                print(f"Completed visualizations for tile size {tile_size}")
                
            except FileNotFoundError as e:
                print(f"Warning: {e}, skipping tile size {tile_size} for video {video_file}")
                continue
            except Exception as e:
                print(f"Error processing tile size {tile_size} for video {video_file}: {e}")
                continue


if __name__ == '__main__':
    main(parse_args())
